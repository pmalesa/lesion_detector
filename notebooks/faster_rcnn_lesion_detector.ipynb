{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad82646",
   "metadata": {},
   "source": [
    "# Faster R-CNN Lesion Detector\n",
    "\n",
    "This notebook is dedicated to training and evaluating a lesion detector on DeepLesion dataset with the Faster R-CNN architecture for image detection with ResNet-50 backbone.\n",
    "\n",
    "## Assumptions:\n",
    "- Use 2D slice inputs (optionally use the neighbouring ones too),\n",
    "- Resize all images to 512x512,\n",
    "- Use YOLO-style Dataset class, with slight modification (bbox coordinates are: [x1, y1, x2, y2]).\n",
    "- Use DeepLesion for training a general lesion localizer and some other like LiTS (Liver Tumor Segmentation) or CHAOS (CT liver dataset) for more specialized localizer.\n",
    "\n",
    "## ðŸ“š Thesis Value Summary\n",
    "### Contribution and Value:\n",
    "- Comparison of 1-stage vs 2-stage vs Transformer vs legacy detectors on DeepLesion\t-> âœ… Fills a gap in literature\n",
    "- Evaluation of improved DETRs (DINO/Deformable) -> âœ… Modern insight\n",
    "- General vs specialized lesion detection -> âœ… Strong clinical relevance\n",
    "- Analysis of training time, robustness, failure modes -> âœ… Engineering depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac67d88",
   "metadata": {},
   "source": [
    "# Google Colab only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b5a33",
   "metadata": {},
   "source": [
    "### Download required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8b79fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/pmalesa/lesion_detector/main/notebooks/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf8a0f5",
   "metadata": {},
   "source": [
    "### Mount DeepLesion images and checkpoints from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e80df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content\n",
    "\n",
    "# remove existing link if any\n",
    "!rm -rf data/deeplesion\n",
    "!rm -rf faster_rcnn_checkpoints\n",
    "\n",
    "!mkdir -p data\n",
    "!ln -s /content/drive/MyDrive/deeplesion/data/deeplesion data/deeplesion\n",
    "!ln -s /content/drive/MyDrive/deeplesion/checkpoints/faster_rcnn faster_rcnn_checkpoints\n",
    "!ls -l data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1901fb48",
   "metadata": {},
   "source": [
    "# Import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Faster R-CNN packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import torchvision.ops as ops\n",
    "import copy\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.models.detection.rpn import RPNHead\n",
    "from torchvision.ops import box_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e661233b",
   "metadata": {},
   "source": [
    "# Set paths to DeepLesion images and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9babc7b4",
   "metadata": {},
   "source": [
    "## Paths to unprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55543a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab\n",
    "deeplesion_metadata_path = Path(\"data/deeplesion/deeplesion_metadata.csv\")\n",
    "deeplesion_image_path = Path(\"data/deeplesion/key_slices/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60672042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local\n",
    "deeplesion_metadata_path = Path(\"../data/deeplesion_metadata.csv\")\n",
    "deeplesion_image_path = Path(\"../data/deeplesion/key_slices/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41ebe9f",
   "metadata": {},
   "source": [
    "## Paths to processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b15776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplesion_data_dir = Path(\"data/deeplesion/\")\n",
    "deeplesion_preprocessed_image_path = deeplesion_data_dir / \"deeplesion_preprocessed_uint8/key_slices\"\n",
    "deeplesion_preprocessed_metadata_path = deeplesion_data_dir / \"deeplesion_preprocessed_uint8/deeplesion_metadata_preprocessed.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5a65b1",
   "metadata": {},
   "source": [
    "# Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2026fd4",
   "metadata": {},
   "source": [
    "## FROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e01919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_froc_curve(model, loader, device, iou_thr=0.5, score_thresholds=None):\n",
    "    \"\"\"\n",
    "    Computes FROC curve: sensitivity (recall) vs. FP per image.\n",
    "    Returns:\n",
    "        fp_per_image:     np.ndarray of shape [T]\n",
    "        sensitivity:      np.ndarray of shape [T]\n",
    "        score_thresholds: np.ndarray of shape [T] \n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if score_thresholds is None:\n",
    "        # 0.0 ... 1.0, 101 points\n",
    "        score_thresholds = np.linspace(0.0, 1.0, 101)\n",
    "    \n",
    "    # Collect all predictions and GTs first to avoid calling model many times\n",
    "    all_image_preds = []\n",
    "    all_image_gts = []\n",
    "    n_images = 0\n",
    "    n_gt_total = 0\n",
    "\n",
    "    for images, targets in loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        outputs = model(images)\n",
    "\n",
    "        for output, target in zip(outputs, targets):\n",
    "            # predictions\n",
    "            boxes_pred = output[\"boxes\"].detach().cpu()\n",
    "            scores_pred = output[\"scores\"].detach().cpu()\n",
    "\n",
    "            # ground truth (class agnostic)\n",
    "            boxes_gt = target[\"boxes\"].detach().cpu()\n",
    "            all_image_preds.append((boxes_pred, scores_pred))\n",
    "            all_image_gts.append(boxes_gt)\n",
    "            n_images += 1\n",
    "            n_gt_total += boxes_gt.shape[0]\n",
    "    \n",
    "    score_thresholds = np.array(score_thresholds, dtype=np.float32)\n",
    "    fp_per_image = np.zeros_like(score_thresholds)\n",
    "    sensitivity = np.zeros_like(score_thresholds)\n",
    "\n",
    "    # For each threshold, count TP/FP over the whole dataset\n",
    "    for i, thr in enumerate(score_thresholds):\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "\n",
    "        for (boxes_pred, scores_pred), boxes_gt in zip(all_image_preds, all_image_gts):\n",
    "            # Filter predictions by score threshold\n",
    "            keep = scores_pred >= thr\n",
    "            boxes_p = boxes_pred[keep]\n",
    "            scores_p = scores_pred[keep]\n",
    "\n",
    "            if boxes_gt.numel() == 0:\n",
    "                # No GT lesions in this image: all predictions are FPs\n",
    "                FP += boxes_p.shape[0]\n",
    "                continue\n",
    "\n",
    "            if boxes_p.numel() == 0:\n",
    "                # No predictions above threshold, but GTs exist -> all missed (FN)\n",
    "                continue\n",
    "\n",
    "            # Sort preds by score (descending) for greedy matching\n",
    "            order = torch.argsort(scores_p, descending=True)\n",
    "            boxes_p = boxes_p[order]\n",
    "\n",
    "            ious = box_iou(boxes_p, boxes_gt) # [N_pred, N_gt]\n",
    "            matched_gt = torch.zeros(boxes_gt.shape[0], dtype=torch.bool)\n",
    "\n",
    "            for p_idx in range(boxes_p.shape[0]):\n",
    "                # Best-match GT for this prediction\n",
    "                iou_vals = ious[p_idx]\n",
    "                best_iou, best_gt_idx = iou_vals.max(0)\n",
    "\n",
    "                if best_iou >= iou_thr and not matched_gt[best_gt_idx]:\n",
    "                    TP += 1\n",
    "                    matched_gt[best_gt_idx] = True\n",
    "                else:\n",
    "                    FP += 1\n",
    "        \n",
    "        sensitivity[i] = TP / max(1, n_gt_total)\n",
    "        fp_per_image[i] = FP / max(1, n_images)\n",
    "\n",
    "    return fp_per_image, sensitivity, score_thresholds\n",
    "\n",
    "\n",
    "def sensitivity_at_fp(fp_per_image, sensitivity, fp_targets):\n",
    "    \"\"\"\n",
    "    Interpolate sensitivity at target FP/image values.\n",
    "    fp_per_image, sensitivity: np arrays from compute_froc_curve()\n",
    "    fp_targets: list or array of target FP/image values, e.g. [0.5, 1, 2, 4]\n",
    "    \n",
    "    Returns dict: {fp_target, sensitivity_value}\n",
    "    \"\"\"\n",
    "\n",
    "    fp_per_image = np.asarray(fp_per_image)\n",
    "    sensitivity = np.asarray(sensitivity)\n",
    "    fp_targets = np.asarray(fp_targets, dtype=np.float32)\n",
    "\n",
    "    # Ensure fp_per_image is sorted ascending\n",
    "    order = np.argsort(fp_per_image)\n",
    "    fp_sorted = fp_per_image[order]\n",
    "    sens_sorted = sensitivity[order]\n",
    "\n",
    "\n",
    "    # Use numpy interpolation\n",
    "    sens_at = np.interp(fp_targets, fp_sorted, sens_sorted, left=0.0, right=sens_sorted[-1])\n",
    "\n",
    "    return {float(fp): float(s) for fp, s in zip(fp_targets, sens_at)}\n",
    "\n",
    "\n",
    "def plot_froc_curve(fp_per_image, sensitivity):\n",
    "    \"\"\"\n",
    "    Plots the full FROC curve.\n",
    "\n",
    "    fp_per_image, sensitivity: np arrays from compute_froc_curve()\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fp_per_image, sensitivity, color=\"red\", marker=\"o\")\n",
    "    plt.xlabel(\"False positives per image\")\n",
    "    plt.ylabel(\"Sensitivity (recall)\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "    plt.title(\"FROC curve (class-agnostic lesions)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_froc_curve_info(model, loader, device):\n",
    "    fp_curve, sens_curve, thr = compute_froc_curve(model, loader, device, iou_thr=0.5)\n",
    "    targets = [0.5, 1.0, 2.0, 4.0, 5.0, 8.0]\n",
    "    sens_dict = sensitivity_at_fp(fp_curve, sens_curve, targets)\n",
    "    plot_froc_curve(fp_curve, sens_curve)\n",
    "    print(\"FROC (class-agnostic):\")\n",
    "    for fp, s in sens_dict.items():\n",
    "        print(f\"Sensitivity at {fp} FP/image: {s*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ebeb73",
   "metadata": {},
   "source": [
    "## Mean Average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c63c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_detector(model, loader, device, num_classes=8, class_names=None, pr_conf_thr=0.25, pr_iou_thr=0.5):\n",
    "    \"\"\"\n",
    "        pr_conf_thr is used to filter predictions by their score.\n",
    "        pr_iou_thr is used to define what is considered a TP in fixed-threshold precision-recall computation.\n",
    "        pr_conf_thr and pr_iou_thr parameters affect only precision and recall calculation, not mAPs.\n",
    "        mAPs are calculated by torchmetrics, which uses its own IoU thresholds.\n",
    "\n",
    "        Returns a dictionary:\n",
    "        {\n",
    "            \"mAP10\": float,\n",
    "            \"mAP30\": float,\n",
    "            \"mAP50\": float,\n",
    "            \"mAP50_95: float,\n",
    "            \"per_class\": [\"name\": ..., \"AP10\": ..., \"AP30\": ..., \"AP50\": ..., \"AP\": ...],\n",
    "            \"precision_overall\": float,\n",
    "            \"recall_overall\": float,\n",
    "            \"precision_per_class\": [...],\n",
    "            \"recall_per_class\": [...],\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    metric_all = MeanAveragePrecision(iou_type=\"bbox\", box_format=\"xyxy\", class_metrics=True)\n",
    "    metric_10  = MeanAveragePrecision(iou_type=\"bbox\", box_format=\"xyxy\", class_metrics=True, iou_thresholds=[0.1])\n",
    "    metric_30  = MeanAveragePrecision(iou_type=\"bbox\", box_format=\"xyxy\", class_metrics=True, iou_thresholds=[0.3])\n",
    "    metric_50  = MeanAveragePrecision(iou_type=\"bbox\", box_format=\"xyxy\", class_metrics=True, iou_thresholds=[0.5])\n",
    "\n",
    "    # For precision and recall at fixed thresholds (IoU=0.5, conf=pr_conf_thr)\n",
    "    TP = torch.zeros(num_classes, dtype=torch.long)\n",
    "    FP = torch.zeros(num_classes, dtype=torch.long)\n",
    "    FN = torch.zeros(num_classes, dtype=torch.long)\n",
    "    n_processed_images = 0\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(loader, start=1):\n",
    "        images = [image.to(device) for image in images]\n",
    "        outputs = model(images)\n",
    "        n_processed_images += len(images)\n",
    "\n",
    "        # Move to CPU for metrics\n",
    "        predictions, ground_truths = [], []\n",
    "        for output, target in zip(outputs, targets):\n",
    "            predictions.append({\"boxes\": output[\"boxes\"].cpu(),\n",
    "                                \"scores\": output[\"scores\"].cpu(),\n",
    "                                \"labels\": output[\"labels\"].cpu()})\n",
    "            ground_truths.append({\"boxes\": target[\"boxes\"].cpu(),\n",
    "                                  \"labels\": target[\"labels\"].cpu()})\n",
    "            \n",
    "        # mAP update\n",
    "        metric_all.update(predictions, ground_truths)\n",
    "        metric_10.update(predictions, ground_truths)\n",
    "        metric_30.update(predictions, ground_truths)\n",
    "        metric_50.update(predictions, ground_truths)\n",
    "\n",
    "        # Precision and recall accumulation at fixed thresholds\n",
    "        for output, target in zip(predictions, ground_truths):\n",
    "            # Filter predictions by confidence\n",
    "            keep = output[\"scores\"] >= pr_conf_thr\n",
    "            scores = output[\"scores\"][keep]\n",
    "            order = torch.argsort(scores, descending=True)\n",
    "            pred_boxes = output[\"boxes\"][keep][order] # Reorder to the same order as scores\n",
    "            pred_labels = output[\"labels\"][keep][order] # Reorder to the same order as scores\n",
    "            gt_boxes = target[\"boxes\"]\n",
    "            gt_labels = target[\"labels\"]\n",
    "\n",
    "            matched = torch.zeros(len(gt_boxes), dtype=torch.bool)\n",
    "            if len(pred_boxes) and len(gt_boxes):\n",
    "                ious = ops.box_iou(pred_boxes, gt_boxes)\n",
    "                for pred_idx in range(len(pred_boxes)):\n",
    "                    cls = int(pred_labels[pred_idx].item()) # classes are 1...K\n",
    "                    # candidates: same class\n",
    "                    same = (gt_labels == cls)\n",
    "                    if same.any():\n",
    "                        ious_c = ious[pred_idx, same]\n",
    "                        if len(ious_c):\n",
    "                            gt_idxs = torch.where(same)[0]\n",
    "                            best_iou, best_loc = ious_c.max(0)\n",
    "                            gt_idx = gt_idxs[best_loc]\n",
    "                            if best_iou >= pr_iou_thr and not matched[gt_idx]:\n",
    "                                TP[cls - 1] += 1\n",
    "                                matched[gt_idx] = True\n",
    "                            else:\n",
    "                                FP[cls - 1] += 1\n",
    "                        else:\n",
    "                            FP[cls - 1] += 1\n",
    "                    else:\n",
    "                        FP[cls - 1] += 1\n",
    "            \n",
    "            # Any unmatched ground truths are FN\n",
    "            for gt_idx, gt_label in enumerate(gt_labels):\n",
    "                if not matched[gt_idx]:\n",
    "                    FN[int(gt_label.item()) - 1] += 1\n",
    "\n",
    "        if batch_idx % 10 == 0 or batch_idx == len(loader):\n",
    "            print(f\"\\r[{n_processed_images}/{len(loader.dataset)}] images validated.\", end=\"\", flush=True)\n",
    "\n",
    "    print() # print a new line\n",
    "\n",
    "    # mAP metrics\n",
    "    res_all = metric_all.compute()\n",
    "    res_10  = metric_10.compute()\n",
    "    res_30  = metric_30.compute()\n",
    "    res_50  = metric_50.compute()\n",
    "\n",
    "    out = {\n",
    "        \"mAP10\": float(res_10[\"map\"]),\n",
    "        \"mAP30\": float(res_30[\"map\"]),\n",
    "        \"mAP50\": float(res_50[\"map\"]),\n",
    "        \"mAP50_95\": float(res_all[\"map\"]),\n",
    "        \"mAP_S\": float(res_all[\"map_small\"]),\n",
    "        \"mAP_M\": float(res_all[\"map_medium\"]),\n",
    "        \"mAP_L\": float(res_all[\"map_large\"])\n",
    "    }\n",
    "\n",
    "    # Per-class AP (if available)\n",
    "    per_class = []\n",
    "    map_per_class = res_all.get(\"map_per_class\", None)\n",
    "    map10_per_class = res_10.get(\"map_per_class\", None)\n",
    "    map30_per_class = res_30.get(\"map_per_class\", None)\n",
    "    map50_per_class = res_50.get(\"map_per_class\", None)\n",
    "    if map_per_class is not None:\n",
    "        ap   = map_per_class.tolist()\n",
    "        ap10 = map10_per_class.tolist()\n",
    "        ap30 = map30_per_class.tolist()\n",
    "        ap50 = map50_per_class.tolist()\n",
    "        for i in range(len(ap)):\n",
    "            name = class_names[i] if class_names and i < len(class_names) else f\"class_{i + 1}\"\n",
    "            per_class.append({\n",
    "                \"idx\": i + 1,\n",
    "                \"name\": name,\n",
    "                \"AP\": _nan_if_undefined(ap[i]), \n",
    "                \"AP10\": _nan_if_undefined(ap10[i]),\n",
    "                \"AP30\": _nan_if_undefined(ap30[i]),\n",
    "                \"AP50\": _nan_if_undefined(ap50[i])\n",
    "            })\n",
    "\n",
    "    out[\"per_class\"] = per_class\n",
    "\n",
    "    # Precision/Recall at fixed thresholds\n",
    "    precision_per_class = (TP.float() / (TP + FP).clamp(min=1)).tolist()\n",
    "    recall_per_class    = (TP.float() / (TP + FN).clamp(min=1)).tolist()\n",
    "    overall_precision   = float(TP.sum() / (TP.sum() + FP.sum()).clamp(min=1))\n",
    "    overall_recall      = float(TP.sum() / (TP.sum() + FN.sum()).clamp(min=1))\n",
    "\n",
    "    out[\"precision_overall\"]   = overall_precision\n",
    "    out[\"recall_overall\"]      = overall_recall\n",
    "    out[\"precision_per_class\"] = precision_per_class\n",
    "    out[\"recall_per_class\"]    = recall_per_class\n",
    "    out[\"pr_conf_thr\"]         = pr_conf_thr\n",
    "    out[\"pr_iou_thr\"]          = pr_iou_thr\n",
    "\n",
    "    return out\n",
    "\n",
    "# =================================================================================================================================================\n",
    "# =================================================================================================================================================\n",
    "\n",
    "def _count_instances_per_class(dataset, num_classes):\n",
    "    \"\"\"\n",
    "    Function that counts the ground truth instances per class by reading the label .txt files on disk.\n",
    "    \"\"\"\n",
    "\n",
    "    counts = [0] * num_classes\n",
    "    total = 0\n",
    "\n",
    "    for img_name in dataset.image_names:\n",
    "        label_path = os.path.join(dataset.label_dir, Path(img_name).stem + \".txt\")\n",
    "        if not os.path.exists(label_path):\n",
    "            continue\n",
    "        with open(label_path, \"r\") as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5: # Ill-written label text file\n",
    "                    continue\n",
    "                cls = int(float(parts[0]))\n",
    "                # Faster R-CNN labels are 1...K (background is implicit), we map to 0...K-1 index\n",
    "                if 1 <= cls <= num_classes:\n",
    "                    counts[cls - 1] += 1\n",
    "                    total += 1\n",
    "    return counts, total\n",
    "\n",
    "def _to_float(x, default=float(\"nan\")):\n",
    "    if x is None:\n",
    "        return default\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        if x.numel() == 0:\n",
    "            return default\n",
    "        x = x.detach().cpu().item() if x.ndim == 0 else x.detach().cpu().numpy()\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return float(x.item()) if x.size == 1 else default\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "    \n",
    "def _to_int(x, default=0):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "    \n",
    "def _nan_if_undefined(x):\n",
    "    return float(\"nan\") if x is None or (isinstance(x, (float, int)) and x < 0) else float(x)\n",
    "\n",
    "def print_result_report(metrics, loader, class_names):\n",
    "    \"\"\"\n",
    "    Function that prints pretty report with evaluation metrics.\n",
    "    Uses dataset files to compute number of images and instances.\n",
    "    \"\"\"\n",
    "\n",
    "    num_classes = len(class_names)\n",
    "    images      = _to_int(len(loader.dataset))\n",
    "    per_class   = metrics.get(\"per_class\", [])\n",
    "    p_overall   = _to_float(metrics[\"precision_overall\"])\n",
    "    r_overall   = _to_float(metrics[\"recall_overall\"])\n",
    "    map10       = _to_float(metrics[\"mAP10\"])\n",
    "    map30       = _to_float(metrics[\"mAP30\"])\n",
    "    map50       = _to_float(metrics[\"mAP50\"])\n",
    "    map50_95    = _to_float(metrics[\"mAP50_95\"])\n",
    "    map_S       = _to_float(metrics[\"mAP_S\"])\n",
    "    map_M       = _to_float(metrics[\"mAP_M\"])\n",
    "    map_L       = _to_float(metrics[\"mAP_L\"])\n",
    "\n",
    "    # Count instances per class from labels\n",
    "    counts, total_instances = _count_instances_per_class(loader.dataset, num_classes)\n",
    "\n",
    "    # Build quick dicts for per-class AP10/AP30/AP50/AP\n",
    "    ap10_by_name = {d['name']: d['AP10'] for d in per_class}\n",
    "    ap30_by_name = {d['name']: d['AP30'] for d in per_class}\n",
    "    ap50_by_name = {d['name']: d['AP50'] for d in per_class}\n",
    "    ap_by_name = {d['name']: d['AP'] for d in per_class}\n",
    "\n",
    "    # Header\n",
    "    print(f\"{'Class':>18} {'Images':>8} {'Instances':>10} {'P':>10} {'R':>10} {'mAP10':>10} {'mAP30':>10} {'mAP50':>10} {'mAP50-95':>10} {'mAPS':>10} {'mAPM':>10} {'mAPL':>10}\")\n",
    "\n",
    "    # Overall row (\"all\")\n",
    "    print(f\"{'all':>18} {images:8d} {_to_int(total_instances):10d} {p_overall:10.3f} {r_overall:10.3f} {map10:10.3f} {map30:10.3f} {map50:10.3f} {map50_95:10.3f} {map_S:10.3f} {map_M:10.3f} {map_L:10.3f}\")\n",
    "\n",
    "    # Per-class rows\n",
    "    p_pc = metrics.get(\"precision_per_class\", [])\n",
    "    r_pc = metrics.get(\"recall_per_class\", [])\n",
    "\n",
    "    for i, name in enumerate(class_names):\n",
    "        P_i = _to_float(p_pc[i] if i < len(p_pc) else float(\"nan\"))\n",
    "        R_i = _to_float(r_pc[i] if i < len(r_pc) else float(\"nan\"))\n",
    "        AP10_i = _to_float(ap10_by_name.get(name, float(\"nan\")))\n",
    "        AP30_i = _to_float(ap30_by_name.get(name, float(\"nan\")))\n",
    "        AP50_i = _to_float(ap50_by_name.get(name, float(\"nan\")))\n",
    "        AP_i = _to_float(ap_by_name.get(name, float(\"nan\")))\n",
    "        inst_i = _to_int(counts[i])\n",
    "        print(f\"{name:>18} {images:8d} {inst_i:10d} {P_i:10.3f} {R_i:10.3f} {AP10_i:10.3f} {AP30_i:10.3f} {AP50_i:10.3f} {AP_i:10.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd31f2b",
   "metadata": {},
   "source": [
    "# Faster R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cad7ac",
   "metadata": {},
   "source": [
    "## Load Pre-trained Faster R-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff0c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes (no. dataset classes + 1 for background)\n",
    "num_classes = 8 + 1\n",
    "class_names = [\"bone\", \"abdomen\", \"mediastinum\", \"liver\", \"lung\", \"kidney\", \"soft_tissue\", \"pelvis\"]\n",
    "\n",
    "# Set up the available device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Set up the default anchor generator\n",
    "def _deeplesion_anchorgen():\n",
    "    # Default: ((32,), (64,), (128,), (256,), (512,)) these subtuples are per level\n",
    "    # anchor_sizes = ((8, 12), (16, 24), (32, 48), (64, 96), (128, 192))\n",
    "    # DeepLesions sizes: ((16,), (24,), (32,), (48,), (96,))\n",
    "    anchor_sizes = ((16,), (24,), (32,), (48,), (96,))\n",
    "    aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "    return AnchorGenerator(anchor_sizes, aspect_ratios)\n",
    "\n",
    "def construct_fasterrcnn_model():\n",
    "    # 1) Load COCO-pretrained Faster R-CNN\n",
    "    weights = FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
    "\n",
    "    # Unfreeze 2 last layers\n",
    "    model = fasterrcnn_resnet50_fpn_v2(\n",
    "        weights=weights,\n",
    "        min_size=512, \n",
    "        max_size=512,\n",
    "        trainable_backbone_layers=2\n",
    "    )\n",
    "\n",
    "    # 2) Swap custom anchor generator\n",
    "    # rpn_anchor_generator = _deeplesion_anchorgen()\n",
    "    # model.rpn.anchor_generator = rpn_anchor_generator\n",
    "\n",
    "    # (OPTIONAL) Tweak proposal counts for small lesions\n",
    "    # model.rpn.pre_nms_top_n_train  = 4000\n",
    "    # model.rpn.post_nms_top_n_train = 2000\n",
    "    # model.rpn.pre_nms_top_n_test   = 2000\n",
    "    # model.rpn.post_nms_top_n_test  = 1000\n",
    "\n",
    "    # (WARNING) Run these lines below if different than default numbers of anchors per level are used\n",
    "    # num_anchors = rpn_anchor_generator.num_anchors_per_location()[0]\n",
    "    # in_channels = model.backbone.out_channels\n",
    "    # model.rpn.head = RPNHead(in_channels, num_anchors)\n",
    "\n",
    "    # Sanity check (prints trainable layers)\n",
    "    # print(\"[LAYERS INFO]\")\n",
    "    # for n, p in model.backbone.body.named_parameters():\n",
    "    #     print(f\"{n} trainable = {p.requires_grad}\")\n",
    "\n",
    "    # 3) Replace the detection head to match the DeepLesion's number of classes\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # 4) Patch the first conv layer to accept 1-channel input\n",
    "    #    (model.backbone.body is the ResNet-50 backbone)\n",
    "    old_conv = model.backbone.body.conv1 # shape: [out_c, 3, k, k] ( == [out_channels, in_channels, kernel_height, kernel_width])\n",
    "\n",
    "    new_conv = nn.Conv2d(\n",
    "        in_channels=1,\n",
    "        out_channels=old_conv.out_channels,\n",
    "        kernel_size=old_conv.kernel_size,\n",
    "        stride=old_conv.stride,\n",
    "        padding=old_conv.padding,\n",
    "        bias=False,\n",
    "    )\n",
    "\n",
    "    # Initialize 1-channel conv layer using pretrained RGB weights\n",
    "    with torch.no_grad():\n",
    "        # Option A: simple average over RGB\n",
    "        new_conv.weight[:] = old_conv.weight.mean(dim=1, keepdim=True)\n",
    "\n",
    "        # Option B: luminance-weighted sum to mimic grayscale\n",
    "        # r = old_conv.weight[:, 0:1, :, :]\n",
    "        # g = old_conv.weight[:, 1:2, :, :]\n",
    "        # b = old_conv.weight[:, 2:3, :, :]\n",
    "        # new_conv.weight[:] = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "    model.backbone.body.conv1 = new_conv\n",
    "\n",
    "    # 5) Adjust the model's internal normalization to 1 channel\n",
    "    # If the loader returns tensors in [0, 1], this centers to roughly ImageNet-like scale.\n",
    "    # [!] If we already pre-normalize to [0, 1] and don't want extra normalization, use mean = [0.0] and std = [1.0].\n",
    "    model.transform.image_mean = [0.5]\n",
    "    model.transform.image_std = [0.5]\n",
    "    # These lists above usually contain 3 values, each for normalization of every RGB channel.\n",
    "    # Since I have only one channel, then I need only one such value in both of these lists.\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    model.to(device)\n",
    "\n",
    "    # Sanity check\n",
    "    # print(f\"First conv layer shape: {model.backbone.body.conv1.weight.shape}\") # Should be [64, 1, 7, 7]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89b6c92",
   "metadata": {},
   "source": [
    "## Data Augmentation - defininition of data transformations classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaab790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComposeTransform:\n",
    "    \"\"\"Compose for (image, target) pairs.\"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "    \n",
    "class ToTensorTransform:\n",
    "    \"\"\"Convert PIL image to tensor, leave target unchanged\"\"\"\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image) # [C, H, W], float32 in [0, 1]\n",
    "        return image, target\n",
    "    \n",
    "class RandomHorizontalFlipTransform:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            # image: [1, H, W] or [C, H, W]\n",
    "            _, h, w = image.shape\n",
    "            image = torch.flip(image, dims=[2]) # flip width dimension\n",
    "\n",
    "            boxes = target[\"boxes\"]\n",
    "            if boxes.numel() > 0:\n",
    "            # boxes: [N, 4] in [x_min, y_min, x_max, y_max]\n",
    "                x_min = boxes[:, 0]\n",
    "                y_min = boxes[:, 1]\n",
    "                x_max = boxes[:, 2]\n",
    "                y_max = boxes[:, 3]\n",
    "\n",
    "                # flip x-coordinates: x' = w - x\n",
    "                new_x_min = w - x_max\n",
    "                new_x_max = w - x_min\n",
    "\n",
    "                boxes = torch.stack([new_x_min, y_min, new_x_max, y_max], dim=1)\n",
    "                target[\"boxes\"] = boxes\n",
    "\n",
    "        return image, target\n",
    "    \n",
    "class RandomBrightnessContrastTransform:\n",
    "    def __init__(self, brightness=0.1, contrast=0.1, p=0.5):\n",
    "        \"\"\"\n",
    "        Relative change of brightness and contrast.\n",
    "        brightness=0.1 means factor in [0.9, 1.1], etc. \n",
    "        \"\"\"\n",
    "        self.brightness = brightness\n",
    "        self.contrast = contrast\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            # image in tensor [C, H, W]\n",
    "            # Random brightness\n",
    "            if self.brightness > 0:\n",
    "                factor = 1.0 + random.uniform(-self.brightness, self.brightness)\n",
    "                image = F.adjust_brightness(image, factor)\n",
    "            # Random contrast\n",
    "            if self.contrast > 0:\n",
    "                factor = 1.0 + random.uniform(-self.contrast, self.contrast)\n",
    "                image = F.adjust_contrast(image, factor)\n",
    "            image = image.clamp(0.0, 1.0)\n",
    "        return image, target\n",
    "    \n",
    "class RandomGaussianNoiseTransform:\n",
    "    def __init__(self, sigma=0.01, p=0.5):\n",
    "        self.sigma = sigma\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            noise = torch.randn_like(image) * self.sigma\n",
    "            image = image + noise\n",
    "            image = image.clamp(0.0, 1.0)\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfdcc68",
   "metadata": {},
   "source": [
    "## Prepare custom Dataset class for Faster R-CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b26fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for DeepLesion dataset\n",
    "class DeepLesionDataset(Dataset):\n",
    "    def __init__(self, root, split):\n",
    "        # Initialize dataset path, split and transformations\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "\n",
    "        # Apply data augmentations only for the train split\n",
    "        if split == \"train\":\n",
    "            self.transforms = ComposeTransform([\n",
    "                ToTensorTransform(),\n",
    "                RandomHorizontalFlipTransform(p=0.5),\n",
    "                RandomBrightnessContrastTransform(brightness=0.1, contrast=0.1, p=0.5),\n",
    "                RandomGaussianNoiseTransform(sigma=0.01, p=0.5),\n",
    "            ])\n",
    "        else:\n",
    "            self.transforms = ComposeTransform([\n",
    "                ToTensorTransform(), # Converts [0, 255] uint8 values to float [0.0, 1.0], and preservers 1 channel\n",
    "            ])\n",
    "\n",
    "        # Dataset logic (image paths, annotations, etc.)\n",
    "        self.image_dir = os.path.join(root, \"images\", split)\n",
    "        self.label_dir = os.path.join(root, \"labels\", split)\n",
    "        self.image_names = sorted([img for img in os.listdir(self.image_dir) if img.endswith(\".png\") or img.endswith(\".jpg\")])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_names[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        label_path = os.path.join(self.label_dir, os.path.splitext(image_name)[0] + \".txt\")\n",
    "\n",
    "        # Load grayscale PIL image\n",
    "        image = Image.open(image_path).convert(\"L\")\n",
    "\n",
    "        # Load corresponding bounding boxes and labels\n",
    "        boxes, labels = [], []\n",
    "        if os.path.exists(label_path):\n",
    "            for line in open(label_path):\n",
    "                cls, x_min, y_min, x_max, y_max = map(float, line.split())\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(int(cls))\n",
    "\n",
    "        # Create a target dictionary\n",
    "        target = {\n",
    "            \"boxes\": torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.as_tensor(labels, dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transforms:\n",
    "            image, target = self.transforms(image, target)\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc32327",
   "metadata": {},
   "source": [
    "## Prepare DataLoader objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba29580c",
   "metadata": {},
   "source": [
    "### Set up the dataset's split path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f244fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fasterrcnn_path = \"deeplesion_fasterrcnn_split_1\" # There are three splits: *_1, *_2 and *_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a0995c",
   "metadata": {},
   "source": [
    "### Set up the batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88262ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab\n",
    "train_batch_size = 4      # Set to 4 (keep 2-4, because that is a sweet spot for two-stage detectors. With higher values may hurt training dynamics)\n",
    "test_val_batch_size = 32   # Set to 32 (high value won't affect metric calculations, but increases memory usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a60ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local\n",
    "train_batch_size = 1\n",
    "test_val_batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f572c89",
   "metadata": {},
   "source": [
    "### Create DataLoader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e96ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Shuffling is enabled for training DataLoader, because SGD benefits from seeing data in a new random order every epoch.\n",
    "  During validation and testing phases we do not need that, the order does not affect the metrics.\n",
    "\n",
    "- num_workers is the number of background processes that load & transorm batches in parallel. Good rule of thumb is num_workers being 2-4.\n",
    "\n",
    "- pin_memory, or pinned (page-locked) host memory, speeds up host to GPU copies and lets us use asynchronous transfers\n",
    "  It should be set to True if we train on GPU. It usually gives a small lbut real throughput bump. It consumes a bit more system RAM\n",
    "  and is useless on CPU-only runs.\n",
    "\n",
    "- Detection models expect lists of images and lists of target dicts, because each image can have different size and has a different\n",
    "  number of boxes. The default PyTorch collate tries to stack everything into tensors of the same shape, which breaks for \n",
    "  variable-length targets. Custom collate_fn function here unzips the list oof pairs into pair of lists so Faster R-CNN can consume them:\n",
    "    images: List[Tensor[C,H,W]]\n",
    "    targets: List[Dict{'boxes': Tensor[N,4], 'labels': Tensor[N]}]\n",
    "  That is exactly what torchvision's detection references use.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train_ds = DeepLesionDataset(deeplesion_data_dir / dataset_fasterrcnn_path, \"train\")\n",
    "val_ds = DeepLesionDataset(deeplesion_data_dir / dataset_fasterrcnn_path, \"val\")\n",
    "test_ds = DeepLesionDataset(deeplesion_data_dir / dataset_fasterrcnn_path, \"test\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch: [(img1, target1), (img2, target2), ...]\n",
    "    # returns: ([img1, img2, ...], [target1, target2, ...])\n",
    "    return tuple(zip(*batch)) # -> \n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=test_val_batch_size, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=test_val_batch_size, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4af3f6",
   "metadata": {},
   "source": [
    "### Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f4e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "- For Faster R-CNN it is common to use SGD or Adam as the optimizer.\n",
    "- Hyperparameters:\n",
    "    - momentum: \n",
    "        adds an exponential moving average of past gradients to the current step, which causes smoother updates,\n",
    "        less zig-zagging and faster convergance. Typically set to 0.9, and rarely needs tuning.\n",
    "    - weight_decay (L2 regularization):\n",
    "        Penalizes large weights to reduce overfitting (shrinks params each step).\n",
    "        Typical for detection with SGD: 5e-4 or 1e-4\n",
    "    - step_size (in StepLR):\n",
    "        Every step_size epochs, the LR scheduler triggers a decay.\n",
    "    - gamma (in StepLR):\n",
    "        Multiplicative LR factor at each step: new_lr = old_lr * gamma. Commonly set to 0.1.\n",
    "\n",
    "- Cross-validate only on the following hyperparameters:\n",
    "    - LR: [0.01, 0.005, 0.002]\n",
    "    - weight_decay: [5e-4, 1e-4]\n",
    "    - Epochs -> don't cross-validate over it -> set a generous cap (e.g. 100) and early stop\n",
    "\n",
    "- use_amp (AMP - Automatic Mixed Precision) - runs many ops in float16 instead of float32, which takes much less GPU memory and is often faster\n",
    "\n",
    "- autocast() is a context manager that automatically picks a safe dtype per op (keeps numerically sensitive ops in float32, others in float16).\n",
    "  It saves memory/computation.\n",
    "  \n",
    "- GradScaler multiplies the loss by a large scale before backprop to avoid float16 underflow, then unscales safely before the optimizer step.\n",
    "  It makes the gradients stable in half precision.\n",
    "\n",
    "- optimizer.zero_grad(set_to_none=True) - set_to_none parameter set to True means that for each parameter param.grad is set to None\n",
    "  (no tensor is kept). On the next backward() PyTorch allocates a fresh grad tensor and writes into it. It causes faster & less memory traffic\n",
    "  by avoiding writing zeros over large grad buffers every step. Lowers memory footprint by letting unused grads be garbage-collected and reallocated\n",
    "  only when needed.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def train_one_config(\n",
    "    train_loader, val_loader, device,\n",
    "    learning_rate, weight_decay, momentum=0.9,\n",
    "    max_epochs=100, patience=15, metric_key=\"mAP50_95\",\n",
    "    gamma=0.1, step_size=3\n",
    "):\n",
    "    # Construct the model\n",
    "    model = construct_fasterrcnn_model()\n",
    "\n",
    "    # =======================================================\n",
    "    # Set up optimizer (different LRs for head and backbone)\n",
    "    # =======================================================\n",
    "    # head_lr = learning_rate\n",
    "    # backbone_lr = learning_rate * 0.1\n",
    "    # head_params = []\n",
    "    # backbone_params = []\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if not param.requires_grad:\n",
    "    #         continue\n",
    "    #     if name.startswith(\"backbone.body\"):\n",
    "    #         backbone_params.append(param)\n",
    "    #     else:\n",
    "    #         head_params.append(param)\n",
    "    \n",
    "    # optimizer = torch.optim.SGD(\n",
    "    #     [\n",
    "    #         {\"params\": backbone_params, \"lr\": backbone_lr, \"weight_decay\": weight_decay},\n",
    "    #         {\"params\": head_params, \"lr\": head_lr, \"weight_decay\": weight_decay},\n",
    "    #     ],\n",
    "    #     momentum=momentum,\n",
    "    # )\n",
    "    # =======================================================\n",
    "\n",
    "    # Set up optimizer (same LR for both head and backbone)\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    # =======================================================\n",
    "    # Learning rate scheduler (StepLR vs MultiStepLR vs CosineAnnealingLR)\n",
    "    # =======================================================\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5, 8, 12], gamma=gamma)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs, eta_min=0.0)\n",
    "    # =======================================================\n",
    "\n",
    "    best_metric = -float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "    history = []\n",
    "    global_step = 0\n",
    "    base_lr = learning_rate\n",
    "\n",
    "    # Set up warmup\n",
    "    n_warmup_epochs = 1\n",
    "    warmup_iters = n_warmup_epochs * len(train_loader)\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        print(f\"\\n*** Epoch [{epoch}/{max_epochs}] started ***\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_processed_images = 0\n",
    "\n",
    "        # Training loop\n",
    "        for batch_idx, (images, targets) in enumerate(train_loader, start=1):\n",
    "            global_step += 1\n",
    "            \n",
    "            # Warmup phase\n",
    "            if global_step <= warmup_iters:\n",
    "                warmup_factor = global_step / float(warmup_iters)\n",
    "                current_lr = base_lr * warmup_factor\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group[\"lr\"] = current_lr\n",
    "                warmup_percent = 100.0 * warmup_factor\n",
    "            else: # let scheduler manage LR\n",
    "                warmup_percent = 100.0\n",
    "\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{key: val.to(device) for key, val in target.items()} for target in targets]\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(loss_dict.values())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n_processed_images += len(images)\n",
    "\n",
    "            if batch_idx % 10 == 0 or batch_idx == len(train_loader):\n",
    "                avg_loss_so_far = running_loss / batch_idx\n",
    "                status = (\n",
    "                    f\"\\rEpoch: [{epoch}/{max_epochs}], \"\n",
    "                    f\"Images: [{n_processed_images}/{len(train_loader.dataset)}], \"\n",
    "                    f\"Warmup: [{float(warmup_percent):.2f}%], \"\n",
    "                    f\"Loss: {avg_loss_so_far:.4f}\"\n",
    "                )\n",
    "                print(status, end=\"\", flush=True)\n",
    "\n",
    "        print()\n",
    "        lr_scheduler.step()\n",
    "        train_loss = running_loss / max(1, len(train_loader))\n",
    "        print(f\"*** Epoch [{epoch}/{max_epochs}] finished -> Loss: {train_loss:.4f} ***\")\n",
    "\n",
    "        # Validation\n",
    "        print(\"*** Validation started ***\")\n",
    "        val_metrics = evaluate_detector(model, val_loader, device, num_classes, class_names)\n",
    "        val_score = float(val_metrics[metric_key])\n",
    "\n",
    "        history.append({\"epoch\": epoch, \"train_loss\": train_loss, **val_metrics})\n",
    "        print(f\"Loss={train_loss:.4f}, mAP50={val_metrics['mAP50']:.4f}, mAP50_95={val_metrics['mAP50_95']:.4f}\")\n",
    "        print(f\"*** Validation finished ***\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_score > best_metric + 1e-6:\n",
    "            best_metric = val_score\n",
    "            best_epoch = epoch\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"*** Early stopping after {epoch}/{max_epochs} epochs (best at {best_epoch} with {metric_key}={best_metric:.4f}). ***\")\n",
    "                break\n",
    "\n",
    "    return {\n",
    "        \"best_metric\": best_metric,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"best_state\": best_state,\n",
    "        \"history\": history,\n",
    "        \"lr\": learning_rate,\n",
    "        \"weight_decay\": weight_decay\n",
    "    }\n",
    "\n",
    "def plot_training_history(result, title=None):\n",
    "    \"\"\"\n",
    "    Plots the history of one training configuration. \n",
    "    result is the dictionary returned by train_one_config\n",
    "    \"\"\"\n",
    "\n",
    "    history = result[\"history\"]\n",
    "    epochs = [ h[\"epoch\"] for h in history ]\n",
    "    train_losses = [ h[\"train_loss\"] for h in history ]\n",
    "    map50 = [ h[\"mAP50\"] for h in history ]\n",
    "    map50_95 = [ h[\"mAP50_95\"] for h in history ]\n",
    "\n",
    "    if title is None:\n",
    "        title = f\"lr={result['lr']}, wd={result['weight_decay']}\"\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "    # Loss on left y-axis\n",
    "    ax1.plot(epochs, train_losses, color=\"blue\", marker=\"o\", label=\"Train loss\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.grid(True, which=\"both\", axis=\"both\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "    # mAP on right y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(epochs, map50, color=\"green\", marker=\"x\", linestyle=\"-\", label=\"mAP50\")\n",
    "    ax2.plot(epochs, map50_95, color=\"red\", marker=\"s\", linestyle=\":\", label=\"mAP50-95\")\n",
    "    ax2.set_ylabel(\"mAP\")\n",
    "\n",
    "    # Combine legends\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"best\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =================================================================================================================================================\n",
    "# =================================================================================================================================================\n",
    "\n",
    "# Set up hyperparameters\n",
    "learning_rates = [0.01] # [0.002, 0.005, 0.01, 0.02]\n",
    "weight_decays = [1e-4] # [1e-4, 5e-4]\n",
    "max_epochs = 30\n",
    "patience = 5\n",
    "best_result = None\n",
    "\n",
    "print(\"*** Training started ***\")\n",
    "for learning_rate in learning_rates:\n",
    "    for weight_decay in weight_decays:\n",
    "        print(f\"[HYPERPARAMETERS]: learning_rate = {learning_rate}, weight_decay = {weight_decay}\")\n",
    "        result = train_one_config(\n",
    "            train_loader=train_loader, val_loader=val_loader, device=device,\n",
    "            learning_rate=learning_rate, weight_decay=weight_decay, max_epochs=max_epochs, patience=patience,\n",
    "            metric_key=\"mAP50_95\"\n",
    "        )\n",
    "\n",
    "        plot_training_history(result)\n",
    "\n",
    "        if best_result is None or result[\"best_metric\"] > best_result[\"best_metric\"]:\n",
    "            best_result = result\n",
    "\n",
    "plot_training_history(best_result, title=f\"[BEST] lr={result['lr']}, wd={result['weight_decay']}\")\n",
    "print(\"*** Training complete ***\")\n",
    "print(f\"Best config: lr={best_result['lr']} wd={best_result['weight_decay']} \"\n",
    "      f\"epoch={best_result['best_epoch']} mAP50_95={best_result['best_metric']:.4f}\")\n",
    "\n",
    "# Save the best model checkpoint\n",
    "best_checkpoint = {\n",
    "    \"state_dict\": best_result[\"best_state\"],\n",
    "    \"epoch\": best_result[\"best_epoch\"],\n",
    "    \"metric_key\": \"mAP50_95\",\n",
    "    \"metric_value\": best_result[\"best_metric\"],\n",
    "    \"hp\": {\n",
    "        \"lr\": best_result[\"lr\"],\n",
    "        \"weight_decay\": best_result[\"weight_decay\"],\n",
    "        \"momentum\": 0.9,\n",
    "        \"step_size\": 3,\n",
    "        \"gamma\": 0.1,\n",
    "        \"max_epochs\": max_epochs,\n",
    "        \"patience\": patience,\n",
    "    },\n",
    "    # Describe how to reconstruct the model\n",
    "    \"model_spec\": {\n",
    "        \"arch\": \"fasterrcnn_resnet50_fpn_v2\",\n",
    "        \"min_size\": 512,\n",
    "        \"max_size\": 512,\n",
    "        \"in_channels\": 1,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"image_mean\": [0.5],\n",
    "        \"image_std\": [0.5],\n",
    "    },\n",
    "    \"class_names\": class_names,\n",
    "    \"versions\": {\"torch\": torch.__version__, \"torchvision\": torchvision.__version__},\n",
    "}\n",
    "\n",
    "# Save checkpoint locally\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "save_path = f\"checkpoints/fasterrcnn_best_{ts}.pt\"\n",
    "torch.save(best_checkpoint, save_path)\n",
    "print(f\"[{ts}] Saved best checkpoint to {save_path}\")\n",
    "\n",
    "# [GOOGLE COLLAB ONLY - uncomment] Save checkpoint on Google drive\n",
    "# gdrive_save_dir = Path(\"faster_rcnn_checkpoints\")\n",
    "# gdrive_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "# gdrive_save_path = os.path.join(str(gdrive_save_dir), f\"fasterrcnn_best_{ts}.pt\")\n",
    "# torch.save(best_checkpoint, gdrive_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bccbb",
   "metadata": {},
   "source": [
    "### Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576ca648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best weights and evaluate on the test set\n",
    "best_checkpoint = torch.load(save_path, map_location=\"cpu\", weights_only=False)\n",
    "best_model = construct_fasterrcnn_model()\n",
    "best_model.load_state_dict(best_checkpoint[\"state_dict\"])\n",
    "\n",
    "# Evaluate the best model\n",
    "print(f\"*** Evaluation started ***\")\n",
    "test_metrics = evaluate_detector(best_model, test_loader, device, num_classes, class_names)\n",
    "\n",
    "print(f\"*** Evaluation finished ***\")\n",
    "print(test_metrics)\n",
    "print_result_report(test_metrics, test_loader, class_names)\n",
    "print_froc_curve_info(best_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ca3e34",
   "metadata": {},
   "source": [
    "# Disconnect Google Colab runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b67b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lesion_detector_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
