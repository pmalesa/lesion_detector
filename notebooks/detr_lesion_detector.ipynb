{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad82646",
   "metadata": {},
   "source": [
    "# DETR Lesion Detector\n",
    "\n",
    "This notebook is dedicated to training and evaluating a lesion detector on DeepLesion dataset with the following supervised model architectures for image detection with ResNet-50 backbone:\n",
    "- DETR (Facebook DETR),\n",
    "- Deformable DETR.\n",
    "\n",
    "## Assumptions:\n",
    "- Use 2D slice inputs (optionally use the neighbouring ones too),\n",
    "- Resize all images to 512x512,\n",
    "- Use YOLO-style Dataset class, with slight modification (bbox coordinates are: [x1, y1, x2, y2]).\n",
    "- Use DeepLesion for training a general lesion localizer and some other like LiTS (Liver Tumor Segmentation) or CHAOS (CT liver dataset) for more specialized localizer.\n",
    "\n",
    "## ðŸ“š Thesis Value Summary\n",
    "### Contribution and Value:\n",
    "- Comparison of 1-stage vs 2-stage vs Transformer vs legacy detectors on DeepLesion\t-> âœ… Fills a gap in literature\n",
    "- Evaluation of improved DETRs (DINO/Deformable) -> âœ… Modern insight\n",
    "- General vs specialized lesion detection -> âœ… Strong clinical relevance\n",
    "- Analysis of training time, robustness, failure modes -> âœ… Engineering depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac67d88",
   "metadata": {},
   "source": [
    "# Google Colab only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b5a33",
   "metadata": {},
   "source": [
    "### Download required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8b79fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/pmalesa/lesion_detector/main/notebooks/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf8a0f5",
   "metadata": {},
   "source": [
    "### Mount DeepLesion images and checkpoints from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e80df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content\n",
    "\n",
    "# remove existing link if any\n",
    "!rm -rf data/deeplesion\n",
    "!rm -rf faster_rcnn_checkpoints\n",
    "\n",
    "!mkdir -p data\n",
    "!ln -s /content/drive/MyDrive/deeplesion/data/deeplesion data/deeplesion\n",
    "!ln -s /content/drive/MyDrive/deeplesion/checkpoints/detr detr_checkpoints\n",
    "!ls -l data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1901fb48",
   "metadata": {},
   "source": [
    "# Import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from typing import Any\n",
    "from PIL import Image\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Faster R-CNN packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import torchvision.ops as ops\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "# DETR packages\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e661233b",
   "metadata": {},
   "source": [
    "# Set paths to DeepLesion images and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1a246",
   "metadata": {},
   "source": [
    "## Paths to unprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c9f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab\n",
    "deeplesion_metadata_path = Path(\"data/deeplesion/deeplesion_metadata.csv\")\n",
    "deeplesion_image_path = Path(\"data/deeplesion/key_slices/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a7e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local\n",
    "deeplesion_metadata_path = Path(\"../data/deeplesion_metadata.csv\")\n",
    "deeplesion_image_path = Path(\"../data/deeplesion/key_slices/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc52423",
   "metadata": {},
   "source": [
    "# Paths to processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b15776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplesion_data_dir = Path(\"data/deeplesion/\")\n",
    "deeplesion_preprocessed_image_path = deeplesion_data_dir / \"deeplesion_preprocessed_uint8/key_slices\"\n",
    "deeplesion_preprocessed_metadata_path = deeplesion_data_dir / \"deeplesion_preprocessed_uint8/deeplesion_metadata_preprocessed.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45397fb3",
   "metadata": {},
   "source": [
    "# Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2026fd4",
   "metadata": {},
   "source": [
    "## FROC Curve Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e01919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_froc_curve(model, loader, device, iou_thr=0.5, score_thresholds=None):\n",
    "    \"\"\"\n",
    "    Computes FROC curve: sensitivity (recall) vs. FP per image.\n",
    "    Returns:\n",
    "        fp_per_image:     np.ndarray of shape [T]\n",
    "        sensitivity:      np.ndarray of shape [T]\n",
    "        score_thresholds: np.ndarray of shape [T] \n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if score_thresholds is None:\n",
    "        # 0.0 ... 1.0, 101 points\n",
    "        score_thresholds = np.linspace(0.0, 1.0, 101)\n",
    "    \n",
    "    # Collect all predictions and GTs first to avoid calling model many times\n",
    "    all_image_preds = []\n",
    "    all_image_gts = []\n",
    "    n_images = 0\n",
    "    n_gt_total = 0\n",
    "\n",
    "    for images, targets in loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        outputs = model(images)\n",
    "\n",
    "        for output, target in zip(outputs, targets):\n",
    "            # predictions\n",
    "            boxes_pred = output[\"boxes\"].detach().cpu()\n",
    "            scores_pred = output[\"scores\"].detach().cpu()\n",
    "\n",
    "            # ground truth (class agnostic)\n",
    "            boxes_gt = target[\"boxes\"].detach().cpu()\n",
    "            all_image_preds.append((boxes_pred, scores_pred))\n",
    "            all_image_gts.append(boxes_gt)\n",
    "            n_images += 1\n",
    "            n_gt_total += boxes_gt.shape[0]\n",
    "    \n",
    "    score_thresholds = np.array(score_thresholds, dtype=np.float32)\n",
    "    fp_per_image = np.zeros_like(score_thresholds)\n",
    "    sensitivity = np.zeros_like(score_thresholds)\n",
    "\n",
    "    # For each threshold, count TP/FP over the whole dataset\n",
    "    for i, thr in enumerate(score_thresholds):\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "\n",
    "        for (boxes_pred, scores_pred), boxes_gt in zip(all_image_preds, all_image_gts):\n",
    "            # Filter predictions by score threshold\n",
    "            keep = scores_pred >= thr\n",
    "            boxes_p = boxes_pred[keep]\n",
    "            scores_p = scores_pred[keep]\n",
    "\n",
    "            if boxes_gt.numel() == 0:\n",
    "                # No GT lesions in this image: all predictions are FPs\n",
    "                FP += boxes_p.shape[0]\n",
    "                continue\n",
    "\n",
    "            if boxes_p.numel() == 0:\n",
    "                # No predictions above threshold, but GTs exist -> all missed (FN)\n",
    "                continue\n",
    "\n",
    "            # Sort preds by score (descending) for greedy matching\n",
    "            order = torch.argsort(scores_p, descending=True)\n",
    "            boxes_p = boxes_p[order]\n",
    "\n",
    "            ious = box_iou(boxes_p, boxes_gt) # [N_pred, N_gt]\n",
    "            matched_gt = torch.zeros(boxes_gt.shape[0], dtype=torch.bool)\n",
    "\n",
    "            for p_idx in range(boxes_p.shape[0]):\n",
    "                # Best-match GT for this prediction\n",
    "                iou_vals = ious[p_idx]\n",
    "                best_iou, best_gt_idx = iou_vals.max(0)\n",
    "\n",
    "                if best_iou >= iou_thr and not matched_gt[best_gt_idx]:\n",
    "                    TP += 1\n",
    "                    matched_gt[best_gt_idx] = True\n",
    "                else:\n",
    "                    FP += 1\n",
    "        \n",
    "        sensitivity[i] = TP / max(1, n_gt_total)\n",
    "        fp_per_image[i] = FP / max(1, n_images)\n",
    "\n",
    "    return fp_per_image, sensitivity, score_thresholds\n",
    "\n",
    "\n",
    "def sensitivity_at_fp(fp_per_image, sensitivity, fp_targets):\n",
    "    \"\"\"\n",
    "    Interpolate sensitivity at target FP/image values.\n",
    "    fp_per_image, sensitivity: np arrays from compute_froc_curve()\n",
    "    fp_targets: list or array of target FP/image values, e.g. [0.5, 1, 2, 4]\n",
    "    \n",
    "    Returns dict: {fp_target, sensitivity_value}\n",
    "    \"\"\"\n",
    "\n",
    "    fp_per_image = np.asarray(fp_per_image)\n",
    "    sensitivity = np.asarray(sensitivity)\n",
    "    fp_targets = np.asarray(fp_targets, dtype=np.float32)\n",
    "\n",
    "    # Ensure fp_per_image is sorted ascending\n",
    "    order = np.argsort(fp_per_image)\n",
    "    fp_sorted = fp_per_image[order]\n",
    "    sens_sorted = sensitivity[order]\n",
    "\n",
    "\n",
    "    # Use numpy interpolation\n",
    "    sens_at = np.interp(fp_targets, fp_sorted, sens_sorted, left=0.0, right=sens_sorted[-1])\n",
    "\n",
    "    return {float(fp): float(s) for fp, s in zip(fp_targets, sens_at)}\n",
    "\n",
    "\n",
    "def plot_froc_curve(fp_per_image, sensitivity):\n",
    "    \"\"\"\n",
    "    Plots the full FROC curve.\n",
    "\n",
    "    fp_per_image, sensitivity: np arrays from compute_froc_curve()\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fp_per_image, sensitivity, color=\"red\", marker=\"o\")\n",
    "    plt.xlabel(\"False positives per image\")\n",
    "    plt.ylabel(\"Sensitivity (recall)\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "    plt.title(\"FROC curve (class-agnostic lesions)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_froc_curve_info(model, loader, device):\n",
    "    fp_curve, sens_curve, thr = compute_froc_curve(model, loader, device, iou_thr=0.5)\n",
    "    targets = [0.5, 1.0, 2.0, 4.0, 5.0, 8.0]\n",
    "    sens_dict = sensitivity_at_fp(fp_curve, sens_curve, targets)\n",
    "    plot_froc_curve(fp_curve, sens_curve)\n",
    "    print(\"FROC (class-agnostic):\")\n",
    "    for fp, s in sens_dict.items():\n",
    "        print(f\"Sensitivity at {fp} FP/image: {s*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fab786",
   "metadata": {},
   "source": [
    "## Mean Average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f274fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_detector(model, loader, device, num_classes=8, class_names=None, pr_conf_thr=0.25, pr_iou_thr=0.5):\n",
    "    \"\"\"\n",
    "        Returns a dictionary:\n",
    "        {\n",
    "            \"mAP50\": float,\n",
    "            \"mAP50_95: float,\n",
    "            \"per_class\": [\"name\": ..., \"AP50\": ..., \"AP\": ...],\n",
    "            \"precision_overall\": float,\n",
    "            \"recall_overall\": float,\n",
    "            \"precision_per_class\": [...],\n",
    "            \"recall_per_class\": [...],\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    metric_all = MeanAveragePrecision(iou_type=\"bbox\", box_format=\"xyxy\", class_metrics=True)\n",
    "    metric_50  = MeanAveragePrecision(iou_type=\"bbox\", box_format=\"xyxy\", class_metrics=True, iou_thresholds=[0.5])\n",
    "\n",
    "    # For precision and recall at fixed thresholds (IoU=0.5, conf=pr_conf_thr)\n",
    "    TP = torch.zeros(num_classes, dtype=torch.long)\n",
    "    FP = torch.zeros(num_classes, dtype=torch.long)\n",
    "    FN = torch.zeros(num_classes, dtype=torch.long)\n",
    "    n_processed_images = 0\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(loader, start=1):\n",
    "        images = [image.to(device) for image in images]\n",
    "        outputs = model(images)\n",
    "        n_processed_images += len(images)\n",
    "\n",
    "        # Move to CPU for metrics\n",
    "        predictions, ground_truths = [], []\n",
    "        for output, target in zip(outputs, targets):\n",
    "            predictions.append({\"boxes\": output[\"boxes\"].cpu(),\n",
    "                                \"scores\": output[\"scores\"].cpu(),\n",
    "                                \"labels\": output[\"labels\"].cpu()})\n",
    "            ground_truths.append({\"boxes\": target[\"boxes\"].cpu(),\n",
    "                                  \"labels\": target[\"labels\"].cpu()})\n",
    "            \n",
    "        # mAP update\n",
    "        metric_all.update(predictions, ground_truths)\n",
    "        metric_50.update(predictions, ground_truths)\n",
    "\n",
    "        # Precision and recall accumulation at fixed thresholds\n",
    "        for output, target in zip(predictions, ground_truths):\n",
    "            # Filter predictions by confidence\n",
    "            keep = output[\"scores\"] >= pr_conf_thr\n",
    "            scores = output[\"scores\"][keep]\n",
    "            order = torch.argsort(scores, descending=True)\n",
    "            pred_boxes = output[\"boxes\"][keep][order] # Reorder to the same order as scores\n",
    "            pred_labels = output[\"labels\"][keep][order] # Reorder to the same order as scores\n",
    "            gt_boxes = target[\"boxes\"]\n",
    "            gt_labels = target[\"labels\"]\n",
    "\n",
    "            matched = torch.zeros(len(gt_boxes), dtype=torch.bool)\n",
    "            if len(pred_boxes) and len(gt_boxes):\n",
    "                ious = ops.box_iou(pred_boxes, gt_boxes)\n",
    "                for pred_idx in range(len(pred_boxes)):\n",
    "                    cls = int(pred_labels[pred_idx].item()) # classes are 1...K\n",
    "                    # candidates: same class\n",
    "                    same = (gt_labels == cls)\n",
    "                    if same.any():\n",
    "                        ious_c = ious[pred_idx, same]\n",
    "                        if len(ious_c):\n",
    "                            gt_idxs = torch.where(same)[0]\n",
    "                            best_iou, best_loc = ious_c.max(0)\n",
    "                            gt_idx = gt_idxs[best_loc]\n",
    "                            if best_iou >= pr_iou_thr and not matched[gt_idx]:\n",
    "                                TP[cls - 1] += 1\n",
    "                                matched[gt_idx] = True\n",
    "                            else:\n",
    "                                FP[cls - 1] += 1\n",
    "                        else:\n",
    "                            FP[cls - 1] += 1\n",
    "                    else:\n",
    "                        FP[cls - 1] += 1\n",
    "            \n",
    "            # Any unmatched ground truths are FN\n",
    "            for gt_idx, gt_label in enumerate(gt_labels):\n",
    "                if not matched[gt_idx]:\n",
    "                    FN[int(gt_label.item()) - 1] += 1\n",
    "\n",
    "        if batch_idx % 10 == 0 or batch_idx == len(loader):\n",
    "            print(f\"\\r[{n_processed_images}/{len(loader.dataset)}] images validated.\", end=\"\", flush=True)\n",
    "\n",
    "    print() # print a new line\n",
    "\n",
    "    # mAP metrics\n",
    "    res_all = metric_all.compute()\n",
    "    res_50  = metric_50.compute()\n",
    "\n",
    "    out = {\n",
    "        \"mAP50\": float(res_50[\"map\"]),\n",
    "        \"mAP50_95\": float(res_all[\"map\"]),\n",
    "    }\n",
    "\n",
    "    # Per-class AP (if available)\n",
    "    per_class = []\n",
    "    map_per_class = res_all.get(\"map_per_class\", None)\n",
    "    map50_per_class = res_50.get(\"map_per_class\", None)\n",
    "    if map_per_class is not None:\n",
    "        ap   = map_per_class.tolist()\n",
    "        ap50 = map50_per_class.tolist()\n",
    "        for i in range(len(ap)):\n",
    "            name = class_names[i] if class_names and i < len(class_names) else f\"class_{i + 1}\"\n",
    "            per_class.append({\"idx\": i + 1, \"name\": name, \"AP\": _nan_if_undefined(ap[i]), \"AP50\": _nan_if_undefined(ap50[i])})\n",
    "\n",
    "    out[\"per_class\"] = per_class\n",
    "\n",
    "    # Precision/Recall at fixed thresholds\n",
    "    precision_per_class = (TP.float() / (TP + FP).clamp(min=1)).tolist()\n",
    "    recall_per_class    = (TP.float() / (TP + FN).clamp(min=1)).tolist()\n",
    "    overall_precision   = float(TP.sum() / (TP.sum() + FP.sum()).clamp(min=1))\n",
    "    overall_recall      = float(TP.sum() / (TP.sum() + FN.sum()).clamp(min=1))\n",
    "\n",
    "    out[\"precision_overall\"]   = overall_precision\n",
    "    out[\"recall_overall\"]      = overall_recall\n",
    "    out[\"precision_per_class\"] = precision_per_class\n",
    "    out[\"recall_per_class\"]    = recall_per_class\n",
    "    out[\"pr_conf_thr\"]         = pr_conf_thr\n",
    "    out[\"pr_iou_thr\"]          = pr_iou_thr\n",
    "\n",
    "    return out\n",
    "\n",
    "# =================================================================================================================================================\n",
    "# =================================================================================================================================================\n",
    "\n",
    "def _count_instances_per_class(dataset, num_classes):\n",
    "    \"\"\"\n",
    "    Function that counts the ground truth instances per class by reading the label .txt files on disk.\n",
    "    \"\"\"\n",
    "\n",
    "    counts = [0] * num_classes\n",
    "    total = 0\n",
    "\n",
    "    for img_name in dataset.image_names:\n",
    "        label_path = os.path.join(dataset.label_dir, Path(img_name).stem + \".txt\")\n",
    "        if not os.path.exists(label_path):\n",
    "            continue\n",
    "        with open(label_path, \"r\") as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5: # Ill-written label text file\n",
    "                    continue\n",
    "                cls = int(float(parts[0]))\n",
    "                # Faster R-CNN labels are 1...K (background is implicit), we map to 0...K-1 index\n",
    "                if 1 <= cls <= num_classes:\n",
    "                    counts[cls - 1] += 1\n",
    "                    total += 1\n",
    "    return counts, total\n",
    "\n",
    "def _to_float(x, default=float(\"nan\")):\n",
    "    if x is None:\n",
    "        return default\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        if x.numel() == 0:\n",
    "            return default\n",
    "        x = x.detach().cpu().item() if x.ndim == 0 else x.detach().cpu().numpy()\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return float(x.item()) if x.size == 1 else default\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "    \n",
    "def _to_int(x, default=0):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "    \n",
    "def _nan_if_undefined(x):\n",
    "    return float(\"nan\") if x is None or (isinstance(x, (float, int)) and x < 0) else float(x)\n",
    "\n",
    "def print_result_report(metrics, loader, class_names):\n",
    "    \"\"\"\n",
    "    Function that prints pretty report with evaluation metrics.\n",
    "    Uses dataset files to compute number of images and instances.\n",
    "    \"\"\"\n",
    "\n",
    "    num_classes = len(class_names)\n",
    "    images      = _to_int(len(loader.dataset))\n",
    "    per_class   = metrics.get(\"per_class\", [])\n",
    "    p_overall   = _to_float(metrics[\"precision_overall\"])\n",
    "    r_overall   = _to_float(metrics[\"recall_overall\"])\n",
    "    map50       = _to_float(metrics[\"mAP50\"])\n",
    "    map50_95    = _to_float(metrics[\"mAP50_95\"])\n",
    "\n",
    "    # Count instances per class from labels\n",
    "    counts, total_instances = _count_instances_per_class(loader.dataset, num_classes)\n",
    "\n",
    "    # Build quick dicts for per-class AP50/AP\n",
    "    ap50_by_name = {d['name']: d['AP50'] for d in per_class}\n",
    "    ap_by_name = {d['name']: d['AP'] for d in per_class}\n",
    "\n",
    "    # Header\n",
    "    print(f\"{'Class':>18} {'Images':>8} {'Instances':>10} {'P':>10} {'R':>10} {'mAP50':>10} {'mAP50-95':>10}\")\n",
    "\n",
    "    # Overall row (\"all\")\n",
    "    print(f\"{'all':>18} {images:8d} {_to_int(total_instances):10d} {p_overall:10.3f} {r_overall:10.3f} {map50:10.3f} {map50_95:10.3f}\")\n",
    "\n",
    "    # Per-class rows\n",
    "    p_pc = metrics.get(\"precision_per_class\", [])\n",
    "    r_pc = metrics.get(\"recall_per_class\", [])\n",
    "\n",
    "    for i, name in enumerate(class_names):\n",
    "        P_i = _to_float(p_pc[i] if i < len(p_pc) else float(\"nan\"))\n",
    "        R_i = _to_float(r_pc[i] if i < len(r_pc) else float(\"nan\"))\n",
    "        AP50_i = _to_float(ap50_by_name.get(name, float(\"nan\")))\n",
    "        AP_i = _to_float(ap_by_name.get(name, float(\"nan\")))\n",
    "        inst_i = _to_int(counts[i])\n",
    "        print(f\"{name:>18} {images:8d} {inst_i:10d} {P_i:10.3f} {R_i:10.3f} {AP50_i:10.3f} {AP_i:10.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89b6c92",
   "metadata": {},
   "source": [
    "## Data Augmentation - defininition of data transformations classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaab790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComposeTransform:\n",
    "    \"\"\"Compose for (image, target) pairs.\"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "    \n",
    "class ToTensorTransform:\n",
    "    \"\"\"Convert PIL image to tensor, leave target unchanged\"\"\"\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image) # [C, H, W], float32 in [0, 1]\n",
    "        return image, target\n",
    "    \n",
    "class RandomHorizontalFlipTransform:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            # image: [1, H, W] or [C, H, W]\n",
    "            _, h, w = image.shape\n",
    "            image = torch.flip(image, dims=[2]) # flip width dimension\n",
    "\n",
    "            boxes = target[\"boxes\"]\n",
    "            if boxes.numel() > 0:\n",
    "            # boxes: [N, 4] in [x_min, y_min, x_max, y_max]\n",
    "                x_min = boxes[:, 0]\n",
    "                y_min = boxes[:, 1]\n",
    "                x_max = boxes[:, 2]\n",
    "                y_max = boxes[:, 3]\n",
    "\n",
    "                # flip x-coordinates: x' = w - x\n",
    "                new_x_min = w - x_max\n",
    "                new_x_max = w - x_min\n",
    "\n",
    "                boxes = torch.stack([new_x_min, y_min, new_x_max, y_max], dim=1)\n",
    "                target[\"boxes\"] = boxes\n",
    "\n",
    "        return image, target\n",
    "    \n",
    "class RandomBrightnessContrastTransform:\n",
    "    def __init__(self, brightness=0.1, contrast=0.1, p=0.5):\n",
    "        \"\"\"\n",
    "        Relative change of brightness and contrast.\n",
    "        brightness=0.1 means factor in [0.9, 1.1], etc. \n",
    "        \"\"\"\n",
    "        self.brightness = brightness\n",
    "        self.contrast = contrast\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            # image in tensor [C, H, W]\n",
    "            # Random brightness\n",
    "            if self.brightness > 0:\n",
    "                factor = 1.0 + random.uniform(-self.brightness, self.brightness)\n",
    "                image = F.adjust_brightness(image, factor)\n",
    "            # Random contrast\n",
    "            if self.contrast > 0:\n",
    "                factor = 1.0 + random.uniform(-self.contrast, self.contrast)\n",
    "                image = F.adjust_contrast(image, factor)\n",
    "            image = image.clamp(0.0, 1.0)\n",
    "        return image, target\n",
    "    \n",
    "class RandomGaussianNoiseTransform:\n",
    "    def __init__(self, sigma=0.01, p=0.5):\n",
    "        self.sigma = sigma\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            noise = torch.randn_like(image) * self.sigma\n",
    "            image = image + noise\n",
    "            image = image.clamp(0.0, 1.0)\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfdcc68",
   "metadata": {},
   "source": [
    "## Prepare DeepLesion dataset for DETR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b26fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for DeepLesion dataset\n",
    "class DeepLesionDataset(Dataset):\n",
    "    def __init__(self, root, split):\n",
    "        # Initialize dataset path, split and transformations\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "\n",
    "        # Apply data augmentations only for the train split\n",
    "        if split == \"train\":\n",
    "            self.transforms = ComposeTransform([\n",
    "                ToTensorTransform(),\n",
    "                RandomHorizontalFlipTransform(p=0.5),\n",
    "                RandomBrightnessContrastTransform(brightness=0.1, contrast=0.1, p=0.5),\n",
    "                RandomGaussianNoiseTransform(sigma=0.01, p=0.5),\n",
    "            ])\n",
    "        else:\n",
    "            self.transforms = ComposeTransform([\n",
    "                ToTensorTransform(), # Converts [0, 255] uint8 values to float [0.0, 1.0], and preservers 1 channel\n",
    "            ])\n",
    "\n",
    "        # Dataset logic (image paths, annotations, etc.)\n",
    "        self.image_dir = os.path.join(root, \"images\", split)\n",
    "        self.label_dir = os.path.join(root, \"labels\", split)\n",
    "        self.image_names = sorted([img for img in os.listdir(self.image_dir) if img.endswith(\".png\") or img.endswith(\".jpg\")])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_names[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        label_path = os.path.join(self.label_dir, os.path.splitext(image_name)[0] + \".txt\")\n",
    "\n",
    "        # Load grayscale PIL image\n",
    "        image = Image.open(image_path).convert(\"L\")\n",
    "\n",
    "        # Load corresponding bounding boxes and labels\n",
    "        boxes, labels = [], []\n",
    "        if os.path.exists(label_path):\n",
    "            for line in open(label_path):\n",
    "                cls, x_min, y_min, x_max, y_max = map(float, line.split())\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(int(cls))\n",
    "\n",
    "        # Create a target dictionary\n",
    "        target = {\n",
    "            \"boxes\": torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.as_tensor(labels, dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transforms:\n",
    "            image, target = self.transforms(image, target)\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc32327",
   "metadata": {},
   "source": [
    "## Prepare DataLoader objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da99fe0",
   "metadata": {},
   "source": [
    "### Set up the dataset's split path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf1673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [!] Same data splits as for Faster R-CNN\n",
    "deeplesion_detr_path = \"deeplesion_fasterrcnn_split_1\" # There are three splits: *_1, *_2 and *_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd2db9",
   "metadata": {},
   "source": [
    "### Set up the batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32192b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab\n",
    "train_batch_size = 4      # Set to 4 (keep 2-4, because that is a sweet spot for two-stage detectors. With higher values may hurt training dynamics)\n",
    "test_val_batch_size = 32   # Set to 32 (high value won't affect metric calculations, but increases memory usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf755a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local\n",
    "train_batch_size = 1\n",
    "test_val_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e96ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Shuffling is enabled for training DataLoader, because SGD benefits from seeing data in a new random order every epoch.\n",
    "  During validation and testing phases we do not need that, the order does not affect the metrics.\n",
    "\n",
    "- num_workers is the number of background processes that load & transorm batches in parallel. Good rule of thumb is num_workers being 2-4.\n",
    "\n",
    "- pin_memory, or pinned (page-locked) host memory, speeds up host to GPU copies and lets us use asynchronous transfers\n",
    "  It should be set to True if we train on GPU. It usually gives a smal lbut real throughput bump. It consumes a bit more system RAM\n",
    "  and is useless on CPU-only runs.\n",
    "\n",
    "- Detection models expect lists of images and lists of target dicts, because each image can have different size and has a different\n",
    "  number of boxes. The default PyTorch collate tries to stack everything into tensors of the same shape, which breaks for \n",
    "  variable-length targets. Custom collate_fn function here unzips the list oof pairs into pair of lists so Faster R-CNN can consume them:\n",
    "    images: List[Tensor[C,H,W]]\n",
    "    targets: List[Dict{'boxes': Tensor[N,4], 'labels': Tensor[N]}]\n",
    "  That is exactly what torchvision's detection references use.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train_ds = DeepLesionDataset(deeplesion_data_dir / deeplesion_detr_path, \"train\")\n",
    "val_ds = DeepLesionDataset(deeplesion_data_dir / deeplesion_detr_path, \"val\")\n",
    "test_ds = DeepLesionDataset(deeplesion_data_dir / deeplesion_detr_path, \"test\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch: [(img1, target1), (img2, target2), ...]\n",
    "    # returns: ([img1, img2, ...], [target1, target2, ...])\n",
    "    return tuple(zip(*batch)) # -> \n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=test_val_batch_size, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=test_val_batch_size, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202e6f1d",
   "metadata": {},
   "source": [
    "# DETR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4af3f6",
   "metadata": {},
   "source": [
    "### Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998ce540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate DETR with pre-trained weights for entire model\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bccbb",
   "metadata": {},
   "source": [
    "### Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e1d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lesion_detector_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
