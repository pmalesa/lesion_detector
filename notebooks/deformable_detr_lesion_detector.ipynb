{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad82646",
   "metadata": {},
   "source": [
    "# Deformable DETR Lesion Detector\n",
    "\n",
    "This notebook is dedicated to training and evaluating a lesion detector on DeepLesion dataset with the Deformable DETR architecture for image detection with ResNet-50 backbone.\n",
    "\n",
    "## Assumptions:\n",
    "- Use 2D slice inputs (optionally use the neighbouring ones too),\n",
    "- Resize all images to 512x512,\n",
    "- Use YOLO-style Dataset class, with slight modification (bbox coordinates are: [x1, y1, x2, y2]).\n",
    "- Use DeepLesion for training a general lesion localizer and some other small dataset for more specialized localizer.\n",
    "\n",
    "## ðŸ“š Thesis Value Summary\n",
    "### Contribution and Value:\n",
    "- Comparison of 1-stage vs 2-stage vs Transformer vs legacy detectors on DeepLesion\t-> âœ… Fills a gap in literature\n",
    "- Evaluation of improved DETR (Deformable) -> âœ… Modern insight\n",
    "- General vs specialized lesion detection -> âœ… Strong clinical relevance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac67d88",
   "metadata": {},
   "source": [
    "# Google Colab only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b5a33",
   "metadata": {},
   "source": [
    "### Download required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8b79fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/pmalesa/lesion_detector/main/notebooks/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf8a0f5",
   "metadata": {},
   "source": [
    "### Mount DeepLesion images and checkpoints from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e80df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content\n",
    "\n",
    "# remove existing link if any\n",
    "!rm -rf data/deeplesion\n",
    "!rm -rf deformable_detr_checkpoints\n",
    "\n",
    "!mkdir -p data\n",
    "!ln -s /content/drive/MyDrive/deeplesion/data/deeplesion data/deeplesion\n",
    "!ln -s /content/drive/MyDrive/deeplesion/checkpoints/deformable_detr deformable_detr_checkpoints\n",
    "!ls -l data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a61e0",
   "metadata": {},
   "source": [
    "# Clone DETR repository (+ PyTorch 2.x fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and rename original Deformable DETR repo\n",
    "!git clone https://github.com/fundamentalvision/Deformable-DETR.git\n",
    "!mv Deformable-DETR deformable_detr\n",
    "\n",
    "# Clone Torch2.xCUDA12 fix repo\n",
    "!git clone https://github.com/Norman-Ou/Deformable-DETR-Torch2.x-cuda12.git\n",
    "\n",
    "# Overwrite models/ops subdirectory\n",
    "!rm -rf deformable_detr/models/ops\n",
    "!mv Deformable-DETR-Torch2.x-cuda12 deformable_detr/models/ops\n",
    "\n",
    "%cd deformable_detr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c3b0c3",
   "metadata": {},
   "source": [
    "# Compile CUDA Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python deformable_detr/models/ops/setup.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1901fb48",
   "metadata": {},
   "source": [
    "# Import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import copy\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# PyTorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as F\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import torchvision.ops as ops\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "# Deformable DETR packages\n",
    "import sys\n",
    "sys.path.append(\"deformable_detr\")\n",
    "\n",
    "# repo root\n",
    "# sys.path.insert(0, \"/content/deformable_detr\")\n",
    "\n",
    "# # CUDA extension location\n",
    "# sys.path.insert(0, \"/content/deformable_detr/models/ops\")\n",
    "\n",
    "from deformable_detr.models.matcher import HungarianMatcher\n",
    "from deformable_detr.models.deformable_detr import DeformableDETR, SetCriterion\n",
    "from deformable_detr.models.backbone import build_backbone\n",
    "from deformable_detr.models.deformable_transformer import build_deforamble_transformer\n",
    "from deformable_detr.util.misc import NestedTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e661233b",
   "metadata": {},
   "source": [
    "# Set paths to DeepLesion images and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1a246",
   "metadata": {},
   "source": [
    "## Paths to unprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c9f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab\n",
    "deeplesion_metadata_path = Path(\"data/deeplesion/deeplesion_metadata.csv\")\n",
    "deeplesion_image_path = Path(\"data/deeplesion/key_slices/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a7e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local\n",
    "deeplesion_metadata_path = Path(\"../data/deeplesion_metadata.csv\")\n",
    "deeplesion_image_path = Path(\"../data/deeplesion/key_slices/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc52423",
   "metadata": {},
   "source": [
    "# Paths to processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b15776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplesion_data_dir = Path(\"data/deeplesion/\")\n",
    "deeplesion_preprocessed_image_path = deeplesion_data_dir / \"deeplesion_preprocessed_uint8/key_slices\"\n",
    "deeplesion_preprocessed_metadata_path = deeplesion_data_dir / \"deeplesion_preprocessed_uint8/deeplesion_metadata_preprocessed.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e6fec8",
   "metadata": {},
   "source": [
    "# [DEBUG] DETR Criterion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938dfd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_criterion(model, num_classes, device):\n",
    "    matcher = HungarianMatcher(cost_class=1.0, cost_bbox=5.0, cost_giou=2.0)\n",
    "\n",
    "    base_weight_dict = {\"loss_ce\": 1.0, \"loss_bbox\": 5.0, \"loss_giou\": 2.0}\n",
    "\n",
    "    # DETR uses auxiliary losses from intermediate decoder layers by default\n",
    "    num_decoder_layers = model.transformer.decoder.num_layers\n",
    "    weight_dict = dict(base_weight_dict)\n",
    "    for i in range(num_decoder_layers - 1):\n",
    "        weight_dict.update({k + f\"_{i}\": v for k, v in base_weight_dict.items()})\n",
    "\n",
    "    criterion = SetCriterion(\n",
    "        num_classes=num_classes - 1, # foreground only\n",
    "        matcher=matcher,\n",
    "        weight_dict=weight_dict,\n",
    "        eos_coef=0.1,\n",
    "        losses=[\"labels\", \"boxes\"]\n",
    "    )\n",
    "    criterion.to(device)\n",
    "\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45397fb3",
   "metadata": {},
   "source": [
    "# Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be81530",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed9cd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_instances_per_class(dataset, num_classes):\n",
    "    \"\"\"\n",
    "    Function that counts the ground truth instances per class by reading the label .txt files on disk.\n",
    "    \"\"\"\n",
    "\n",
    "    counts = [0] * num_classes\n",
    "    total = 0\n",
    "\n",
    "    for img_name in dataset.image_names:\n",
    "        label_path = os.path.join(dataset.label_dir, Path(img_name).stem + \".txt\")\n",
    "        if not os.path.exists(label_path):\n",
    "            continue\n",
    "        with open(label_path, \"r\") as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5: # Ill-written label text file\n",
    "                    continue\n",
    "                cls = int(float(parts[0]))\n",
    "                # Label files are 1-based (1...K), map to 0...K-1 indices\n",
    "                if 1 <= cls <= num_classes:\n",
    "                    counts[cls - 1] += 1\n",
    "                    total += 1\n",
    "    return counts, total\n",
    "\n",
    "def _to_float(x, default=float(\"nan\")):\n",
    "    if x is None:\n",
    "        return default\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        if x.numel() == 0:\n",
    "            return default\n",
    "        x = x.detach().cpu().item() if x.ndim == 0 else x.detach().cpu().numpy()\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return float(x.item()) if x.size == 1 else default\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "    \n",
    "def _to_int(x, default=0):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "    \n",
    "def _nan_if_undefined(x):\n",
    "    return float(\"nan\") if x is None or (isinstance(x, (float, int)) and x < 0) else float(x)\n",
    "\n",
    "def _cxcywh_to_xyxy(boxes):\n",
    "    \"\"\"\n",
    "    Function to convert boxes' coordinates\n",
    "    from cxcywh to xyxy.\n",
    "\n",
    "    boxes: [B, 4] normalized cxcywh\n",
    "    \"\"\"\n",
    "\n",
    "    cx, cy, w, h = boxes.unbind(-1)\n",
    "    x1 = cx - 0.5 * w\n",
    "    y1 = cy - 0.5 * h\n",
    "    x2 = cx + 0.5 * w\n",
    "    y2 = cy + 0.5 * h\n",
    "    return torch.stack([x1, y1, x2, y2], dim=-1)\n",
    "\n",
    "def _scale_boxes_to_pixels(boxes_xyxy_norm, sizes):\n",
    "    \"\"\"\n",
    "    Function to scale normalized xyxy boxes\n",
    "    to the correct pixel sizes.\n",
    "\n",
    "    boxes_xyxy_norm: [N, 4] in [0, 1]\n",
    "    sizes: (height, width)\n",
    "    \"\"\"\n",
    "\n",
    "    h, w = sizes\n",
    "    scale = boxes_xyxy_norm.new_tensor([w, h, w, h])\n",
    "    return boxes_xyxy_norm * scale\n",
    "\n",
    "def _orig_size_from_mask(mask):\n",
    "    \"\"\"\n",
    "    Function to determine the actual\n",
    "    image sizes from mask.\n",
    "\n",
    "    mask: [h_max, w_max] bool, True=pad, False = valid\n",
    "    \"\"\"\n",
    "\n",
    "    valid = ~mask\n",
    "    h = int(valid.any(dim=1).sum().item())\n",
    "    w = int(valid.any(dim=0).sum().item())\n",
    "    return (h, w)\n",
    "\n",
    "def _detr_outputs_to_predictions(outputs, masks, score_thr=0.0, top_k=100):\n",
    "    \"\"\"\n",
    "    Function to convert DETR outputs to predictions\n",
    "    acceptable by torchvision's methods.\n",
    "\n",
    "    outputs: dict with pred_logits [B, Q, K+1], pred_boxes [B, Q, 4] (cxcywh norm)\n",
    "    masks: [B, height, width] bool\n",
    "    \"\"\"\n",
    "\n",
    "    logits = outputs[\"pred_logits\"]         # [B, Q, K+1]\n",
    "    boxes_cxcywh = outputs[\"pred_boxes\"]    # [B, Q, 4]\n",
    "\n",
    "    # Apply softmax over the class dimension, independently for each query\n",
    "    probs = logits.softmax(-1)              # [B, Q, K+1]\n",
    "    probs_fq = probs[..., :-1]              # drop no-object -> [B, Q, K]\n",
    "    scores, labels = probs_fq.max(-1)       # [B, Q], [B, Q] labels in 0...K-1\n",
    "\n",
    "    preds = []\n",
    "    B, Q = scores.shape\n",
    "    for b in range(B):\n",
    "        # get original image size\n",
    "        height, width = _orig_size_from_mask(masks[b])\n",
    "\n",
    "        # convert boxes to xyxy and scale\n",
    "        boxes_xyxy = _cxcywh_to_xyxy(boxes_cxcywh[b])\n",
    "        boxes_xyxy = boxes_xyxy.clamp(0, 1)\n",
    "        boxes_xyxy = _scale_boxes_to_pixels(boxes_xyxy, (height, width))\n",
    "\n",
    "        # filter / top-k\n",
    "        s = scores[b]\n",
    "        l = labels[b]\n",
    "\n",
    "        keep = s >= score_thr\n",
    "        boxes_xyxy = boxes_xyxy[keep]\n",
    "        s = s[keep]\n",
    "        l = l[keep]\n",
    "\n",
    "        if top_k is not None and boxes_xyxy.shape[0] > top_k:\n",
    "            idx = torch.argsort(s, descending=True)[:top_k]\n",
    "            boxes_xyxy, s, l = boxes_xyxy[idx], s[idx], l[idx]\n",
    "\n",
    "        preds.append({\n",
    "            \"boxes\": boxes_xyxy.cpu(),\n",
    "            \"scores\": s.cpu(),\n",
    "            \"labels\": l.cpu()\n",
    "        })\n",
    "\n",
    "    return preds\n",
    "\n",
    "def _targets_to_ground_truths(targets, masks):\n",
    "    \"\"\"\n",
    "    Function that converts ground truths in targets\n",
    "    from normalized cxcywh to pixel size xyxy.\n",
    "    \"\"\"\n",
    "\n",
    "    gts = []\n",
    "    for idx, target in enumerate(targets):\n",
    "        height, width = _orig_size_from_mask(masks[idx])\n",
    "        boxes_xyxy = _cxcywh_to_xyxy(target[\"boxes\"]).clamp(0, 1)\n",
    "        boxes_xyxy = _scale_boxes_to_pixels(boxes_xyxy, (height, width))\n",
    "        labels = target[\"labels\"]\n",
    "        gts.append({\n",
    "            \"boxes\": boxes_xyxy.cpu(),\n",
    "            \"labels\": labels.cpu()\n",
    "        })\n",
    "\n",
    "    return gts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2026fd4",
   "metadata": {},
   "source": [
    "## FROC Curve Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e01919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_froc_curve(model, loader, device, iou_thr=0.5, score_thresholds=None):\n",
    "    \"\"\"\n",
    "    Computes FROC curve: sensitivity (recall) vs. FP per image.\n",
    "    Returns:\n",
    "        fp_per_image:     np.ndarray of shape [T]\n",
    "        sensitivity:      np.ndarray of shape [T]\n",
    "        score_thresholds: np.ndarray of shape [T] \n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if score_thresholds is None:\n",
    "        # 0.0 ... 1.0, 101 points\n",
    "        score_thresholds = np.linspace(0.0, 1.0, 101)\n",
    "    \n",
    "    # Collect all predictions and GTs first to avoid calling model many times\n",
    "    all_image_preds = []\n",
    "    all_image_gts = []\n",
    "    n_images = 0\n",
    "    n_gt_total = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(loader):\n",
    "        # -- Unpack DETR batch\n",
    "        (images, masks), targets = batch\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        targets = [{key: val.to(device) for key, val in target.items()} for target in targets]\n",
    "\n",
    "        # -- Wrap into NestedTensor (facebook DETR expects this)\n",
    "        nested_tensor = NestedTensor(images, masks)\n",
    "\n",
    "        # -- Forward\n",
    "        outputs = model(nested_tensor)\n",
    "\n",
    "        # -- Postprocess DETR outputs and move to CPU for metrics\n",
    "        predictions = _detr_outputs_to_predictions(outputs, masks, score_thr=0.0, top_k=300)\n",
    "        ground_truths = _targets_to_ground_truths(targets, masks)\n",
    "\n",
    "        for output, target in zip(predictions, ground_truths):\n",
    "            # predictions\n",
    "            boxes_pred = output[\"boxes\"].detach().cpu()\n",
    "            scores_pred = output[\"scores\"].detach().cpu()\n",
    "\n",
    "            # ground truth (class agnostic)\n",
    "            boxes_gt = target[\"boxes\"].detach().cpu()\n",
    "\n",
    "            all_image_preds.append((boxes_pred, scores_pred))\n",
    "            all_image_gts.append(boxes_gt)\n",
    "            \n",
    "            n_images += 1\n",
    "            n_gt_total += boxes_gt.shape[0]\n",
    "\n",
    "    score_thresholds = np.array(score_thresholds, dtype=np.float32)\n",
    "    fp_per_image = np.zeros_like(score_thresholds)\n",
    "    sensitivity = np.zeros_like(score_thresholds)\n",
    "\n",
    "    # For each threshold, count TP/FP over the whole dataset\n",
    "    for i, thr in enumerate(score_thresholds):\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "\n",
    "        for (boxes_pred, scores_pred), boxes_gt in zip(all_image_preds, all_image_gts):\n",
    "            # Filter predictions by score threshold\n",
    "            keep = scores_pred >= thr\n",
    "            boxes_p = boxes_pred[keep]\n",
    "            scores_p = scores_pred[keep]\n",
    "\n",
    "            if boxes_gt.numel() == 0:\n",
    "                # No GT lesions in this image: all predictions are FPs\n",
    "                FP += boxes_p.shape[0]\n",
    "                continue\n",
    "\n",
    "            if boxes_p.numel() == 0:\n",
    "                # No predictions above threshold, but GTs exist -> all missed (FN)\n",
    "                continue\n",
    "\n",
    "            # Sort preds by score (descending) for greedy matching\n",
    "            order = torch.argsort(scores_p, descending=True)\n",
    "            boxes_p = boxes_p[order]\n",
    "\n",
    "            ious = box_iou(boxes_p, boxes_gt) # [N_pred, N_gt]\n",
    "            matched_gt = torch.zeros(boxes_gt.shape[0], dtype=torch.bool)\n",
    "\n",
    "            for p_idx in range(boxes_p.shape[0]):\n",
    "                # Best-match GT for this prediction\n",
    "                iou_vals = ious[p_idx]\n",
    "                best_iou, best_gt_idx = iou_vals.max(0)\n",
    "\n",
    "                # one-to-one matching (one prediction <-> one GT)\n",
    "                if best_iou >= iou_thr and not matched_gt[best_gt_idx]:\n",
    "                    TP += 1\n",
    "                    matched_gt[best_gt_idx] = True\n",
    "                else:\n",
    "                    FP += 1\n",
    "        \n",
    "        sensitivity[i] = TP / max(1, n_gt_total)\n",
    "        fp_per_image[i] = FP / max(1, n_images)\n",
    "\n",
    "    return fp_per_image, sensitivity, score_thresholds\n",
    "\n",
    "\n",
    "def sensitivity_at_fp(fp_per_image, sensitivity, fp_targets):\n",
    "    \"\"\"\n",
    "    Interpolate sensitivity at target FP/image values.\n",
    "    fp_per_image, sensitivity: np arrays from compute_froc_curve()\n",
    "    fp_targets: list or array of target FP/image values, e.g. [0.5, 1, 2, 4]\n",
    "    \n",
    "    Returns dict: {fp_target, sensitivity_value}\n",
    "    \"\"\"\n",
    "\n",
    "    fp_per_image = np.asarray(fp_per_image)\n",
    "    sensitivity = np.asarray(sensitivity)\n",
    "    fp_targets = np.asarray(fp_targets, dtype=np.float32)\n",
    "\n",
    "    # Ensure fp_per_image is sorted ascending\n",
    "    order = np.argsort(fp_per_image)\n",
    "    fp_sorted = fp_per_image[order]\n",
    "    sens_sorted = sensitivity[order]\n",
    "\n",
    "    # Use numpy interpolation\n",
    "    sens_at = np.interp(fp_targets, fp_sorted, sens_sorted, left=0.0, right=sens_sorted[-1])\n",
    "\n",
    "    return {float(fp): float(s) for fp, s in zip(fp_targets, sens_at)}\n",
    "\n",
    "def plot_froc_curve(fp_per_image, sensitivity):\n",
    "    \"\"\"\n",
    "    Plots the full FROC curve.\n",
    "\n",
    "    fp_per_image, sensitivity: np arrays from compute_froc_curve()\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fp_per_image, sensitivity, color=\"red\", marker=\"o\")\n",
    "    plt.xlabel(\"False positives per image\")\n",
    "    plt.ylabel(\"Sensitivity (recall)\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "    plt.title(\"FROC curve (class-agnostic lesions)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_froc_curve_info(model, loader, device):\n",
    "    fp_curve, sens_curve, thr = compute_froc_curve(model, loader, device, iou_thr=0.5)\n",
    "    targets = [0.5, 1.0, 2.0, 4.0, 5.0, 8.0]\n",
    "    sens_dict = sensitivity_at_fp(fp_curve, sens_curve, targets)\n",
    "    plot_froc_curve(fp_curve, sens_curve)\n",
    "    print(\"FROC (class-agnostic):\")\n",
    "    for fp, s in sens_dict.items():\n",
    "        print(f\"Sensitivity at {fp} FP/image: {s*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fab786",
   "metadata": {},
   "source": [
    "## Mean Average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f274fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_detector(model, loader, device, num_classes=8, class_names=None, pr_conf_thr=0.25, pr_iou_thr=0.5):\n",
    "    \"\"\"\n",
    "        Returns a dictionary:\n",
    "        {\n",
    "            \"mAP50\": float,\n",
    "            \"mAP50_95: float,\n",
    "            \"per_class\": [\"name\": ..., \"AP50\": ..., \"AP\": ...],\n",
    "            \"precision_overall\": float,\n",
    "            \"recall_overall\": float,\n",
    "            \"precision_per_class\": [...],\n",
    "            \"recall_per_class\": [...],\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    metric_all = MeanAveragePrecision(iou_type=\"bbox\", box_format=\"xyxy\", class_metrics=True)\n",
    "    metric_10  = MeanAveragePrecision(iou_type=\"bbox\", box_format=\"xyxy\", class_metrics=True, iou_thresholds=[0.1])\n",
    "    metric_30  = MeanAveragePrecision(iou_type=\"bbox\", box_format=\"xyxy\", class_metrics=True, iou_thresholds=[0.3])\n",
    "    metric_50  = MeanAveragePrecision(iou_type=\"bbox\", box_format=\"xyxy\", class_metrics=True, iou_thresholds=[0.5])\n",
    "\n",
    "    # For precision and recall at fixed thresholds (IoU=0.5, conf=pr_conf_thr)\n",
    "    TP = torch.zeros(num_classes, dtype=torch.long)\n",
    "    FP = torch.zeros(num_classes, dtype=torch.long)\n",
    "    FN = torch.zeros(num_classes, dtype=torch.long)\n",
    "    n_processed_images = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(loader, start=1):\n",
    "        # -- Unpack DETR batch\n",
    "        (images, masks), targets = batch\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        targets = [{key: val.to(device) for key, val in target.items()} for target in targets]\n",
    "\n",
    "        # -- Wrap into NestedTensor (facebook DETR expects this)\n",
    "        nested_tensor = NestedTensor(images, masks)\n",
    "\n",
    "        # -- Forward\n",
    "        outputs = model(nested_tensor)\n",
    "        n_processed_images += images.size(0)\n",
    "\n",
    "        # DEBUG#############################################################\n",
    "        # DEBUG#############################################################\n",
    "        # criterion = build_criterion(model, num_classes, device)\n",
    "        # matcher = criterion.matcher\n",
    "        # with torch.no_grad():\n",
    "        #     indices = matcher(outputs, targets)\n",
    "        #     ious = []\n",
    "\n",
    "        #     for b, (pred_idx, target_idx) in enumerate(indices):\n",
    "        #         if len(pred_idx) == 0:\n",
    "        #             continue\n",
    "\n",
    "        #         p = outputs[\"pred_boxes\"][b][pred_idx]\n",
    "        #         t = targets[b][\"boxes\"][target_idx]\n",
    "\n",
    "        #         p_xyxy = _cxcywh_to_xyxy(p)\n",
    "        #         t_xyxy = _cxcywh_to_xyxy(t)\n",
    "\n",
    "        #         iou = box_iou(p_xyxy, t_xyxy).diag()\n",
    "        #         ious.append(iou)\n",
    "\n",
    "        #     if ious:\n",
    "        #         print(\"\\nMatched IoU mean: \", torch.cat(ious).mean().item())\n",
    "        # DEBUG#############################################################\n",
    "        # DEBUG#############################################################\n",
    "\n",
    "        # -- Postprocess DETR outputs and move to CPU for metrics\n",
    "        predictions = _detr_outputs_to_predictions(outputs, masks)\n",
    "        ground_truths = _targets_to_ground_truths(targets, masks)\n",
    "\n",
    "        # -- Sanity check\n",
    "        # assert all(0 <= l < 8 for l in torch.cat([g[\"labels\"] for g in ground_truths]))\n",
    "        # assert all(0 <= l < 8 for l in torch.cat([p[\"labels\"] for p in predictions]))\n",
    "\n",
    "        # -- mAP update\n",
    "        metric_all.update(predictions, ground_truths)\n",
    "        metric_10.update(predictions, ground_truths)\n",
    "        metric_30.update(predictions, ground_truths)\n",
    "        metric_50.update(predictions, ground_truths)\n",
    "\n",
    "        # -- Precision and recall accumulation at fixed thresholds\n",
    "        for output, target in zip(predictions, ground_truths):\n",
    "            # ---- Filter predictions by confidence\n",
    "            keep = output[\"scores\"] >= pr_conf_thr\n",
    "            scores = output[\"scores\"][keep]\n",
    "            order = torch.argsort(scores, descending=True)\n",
    "            pred_boxes = output[\"boxes\"][keep][order] # Reorder to the same order as scores\n",
    "            pred_labels = output[\"labels\"][keep][order] # Reorder to the same order as scores\n",
    "            gt_boxes = target[\"boxes\"]\n",
    "            gt_labels = target[\"labels\"]\n",
    "\n",
    "            matched = torch.zeros(len(gt_boxes), dtype=torch.bool)\n",
    "            if len(pred_boxes) and len(gt_boxes):\n",
    "                ious = ops.box_iou(pred_boxes, gt_boxes)\n",
    "                for pred_idx in range(len(pred_boxes)):\n",
    "                    cls = int(pred_labels[pred_idx].item()) # classes are 0...K-1 (foreground only)\n",
    "                    # candidates: same class\n",
    "                    same = (gt_labels == cls)\n",
    "                    if same.any():\n",
    "                        ious_c = ious[pred_idx, same]\n",
    "                        if len(ious_c):\n",
    "                            gt_idxs = torch.where(same)[0]\n",
    "                            best_iou, best_loc = ious_c.max(0)\n",
    "                            gt_idx = gt_idxs[best_loc]\n",
    "                            if best_iou >= pr_iou_thr and not matched[gt_idx]:\n",
    "                                TP[cls] += 1\n",
    "                                matched[gt_idx] = True\n",
    "                            else:\n",
    "                                FP[cls] += 1\n",
    "                        else:\n",
    "                            FP[cls] += 1\n",
    "                    else:\n",
    "                        FP[cls] += 1\n",
    "            \n",
    "            # ---- Any unmatched ground truths are FN\n",
    "            for gt_idx, gt_label in enumerate(gt_labels):\n",
    "                if not matched[gt_idx]:\n",
    "                    FN[int(gt_label.item())] += 1\n",
    "\n",
    "        if batch_idx % 10 == 0 or batch_idx == len(loader):\n",
    "            print(f\"\\r[{n_processed_images}/{len(loader.dataset)}] images validated.\", end=\"\", flush=True)\n",
    "\n",
    "    print() # print a new line\n",
    "\n",
    "    # mAP metrics\n",
    "    res_all = metric_all.compute()\n",
    "    res_10  = metric_10.compute()\n",
    "    res_30  = metric_30.compute()\n",
    "    res_50  = metric_50.compute()\n",
    "\n",
    "    out = {\n",
    "        \"mAP10\": float(res_10[\"map\"]),\n",
    "        \"mAP30\": float(res_30[\"map\"]),\n",
    "        \"mAP50\": float(res_50[\"map\"]),\n",
    "        \"mAP50_95\": float(res_all[\"map\"]),\n",
    "    }\n",
    "\n",
    "    # Per-class AP (if available)\n",
    "    per_class = []\n",
    "    map_per_class = res_all.get(\"map_per_class\", None)\n",
    "    map10_per_class = res_10.get(\"map_per_class\", None)\n",
    "    map30_per_class = res_30.get(\"map_per_class\", None)\n",
    "    map50_per_class = res_50.get(\"map_per_class\", None)\n",
    "    if map_per_class is not None:\n",
    "        ap   = map_per_class.tolist()\n",
    "        ap10 = map10_per_class.tolist()\n",
    "        ap30 = map30_per_class.tolist()\n",
    "        ap50 = map50_per_class.tolist()\n",
    "        for i in range(len(ap)):\n",
    "            name = class_names[i] if class_names and i < len(class_names) else f\"class_{i + 1}\"\n",
    "            per_class.append({\n",
    "                \"idx\": i + 1,\n",
    "                \"name\": name,\n",
    "                \"AP\": _nan_if_undefined(ap[i]), \n",
    "                \"AP10\": _nan_if_undefined(ap10[i]),\n",
    "                \"AP30\": _nan_if_undefined(ap30[i]),\n",
    "                \"AP50\": _nan_if_undefined(ap50[i])\n",
    "            })\n",
    "\n",
    "    out[\"per_class\"] = per_class\n",
    "\n",
    "    # Precision/Recall at fixed thresholds\n",
    "    precision_per_class = (TP.float() / (TP + FP).clamp(min=1)).tolist()\n",
    "    recall_per_class    = (TP.float() / (TP + FN).clamp(min=1)).tolist()\n",
    "    overall_precision   = float(TP.sum() / (TP.sum() + FP.sum()).clamp(min=1))\n",
    "    overall_recall      = float(TP.sum() / (TP.sum() + FN.sum()).clamp(min=1))\n",
    "\n",
    "    out[\"precision_overall\"]   = overall_precision\n",
    "    out[\"recall_overall\"]      = overall_recall\n",
    "    out[\"precision_per_class\"] = precision_per_class\n",
    "    out[\"recall_per_class\"]    = recall_per_class\n",
    "    out[\"pr_conf_thr\"]         = pr_conf_thr\n",
    "    out[\"pr_iou_thr\"]          = pr_iou_thr\n",
    "\n",
    "    return out\n",
    "\n",
    "def print_result_report(metrics, loader, class_names):\n",
    "    \"\"\"\n",
    "    Function that prints pretty report with evaluation metrics.\n",
    "    Uses dataset files to compute number of images and instances.\n",
    "    \"\"\"\n",
    "\n",
    "    num_classes = len(class_names)\n",
    "    images      = _to_int(len(loader.dataset))\n",
    "    per_class   = metrics.get(\"per_class\", [])\n",
    "    p_overall   = _to_float(metrics[\"precision_overall\"])\n",
    "    r_overall   = _to_float(metrics[\"recall_overall\"])\n",
    "    map10       = _to_float(metrics[\"mAP10\"])\n",
    "    map30       = _to_float(metrics[\"mAP30\"])\n",
    "    map50       = _to_float(metrics[\"mAP50\"])\n",
    "    map50_95    = _to_float(metrics[\"mAP50_95\"])\n",
    "\n",
    "    # Count instances per class from labels\n",
    "    counts, total_instances = _count_instances_per_class(loader.dataset, num_classes)\n",
    "\n",
    "    # Build quick dicts for per-class AP10/AP30/AP50/AP\n",
    "    ap10_by_name = {d['name']: d['AP10'] for d in per_class}\n",
    "    ap30_by_name = {d['name']: d['AP30'] for d in per_class}\n",
    "    ap50_by_name = {d['name']: d['AP50'] for d in per_class}\n",
    "    ap_by_name = {d['name']: d['AP'] for d in per_class}\n",
    "\n",
    "    # Header\n",
    "    print(f\"{'Class':>18} {'Images':>8} {'Instances':>10} {'P':>10} {'R':>10} {'mAP10':>10} {'mAP30':>10} {'mAP50':>10} {'mAP50-95':>10}\")\n",
    "\n",
    "    # Overall row (\"all\")\n",
    "    print(f\"{'all':>18} {images:8d} {_to_int(total_instances):10d} {p_overall:10.3f} {r_overall:10.3f} {map10:10.3f} {map30:10.3f} {map50:10.3f} {map50_95:10.3f}\")\n",
    "\n",
    "    # Per-class rows\n",
    "    p_pc = metrics.get(\"precision_per_class\", [])\n",
    "    r_pc = metrics.get(\"recall_per_class\", [])\n",
    "\n",
    "    for i, name in enumerate(class_names):\n",
    "        P_i = _to_float(p_pc[i] if i < len(p_pc) else float(\"nan\"))\n",
    "        R_i = _to_float(r_pc[i] if i < len(r_pc) else float(\"nan\"))\n",
    "        AP10_i = _to_float(ap10_by_name.get(name, float(\"nan\")))\n",
    "        AP30_i = _to_float(ap30_by_name.get(name, float(\"nan\")))\n",
    "        AP50_i = _to_float(ap50_by_name.get(name, float(\"nan\")))\n",
    "        AP_i = _to_float(ap_by_name.get(name, float(\"nan\")))\n",
    "        inst_i = _to_int(counts[i])\n",
    "        print(f\"{name:>18} {images:8d} {inst_i:10d} {P_i:10.3f} {R_i:10.3f} {AP10_i:10.3f} {AP30_i:10.3f} {AP50_i:10.3f} {AP_i:10.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89b6c92",
   "metadata": {},
   "source": [
    "## Data Augmentation - defininition of data transformations classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaab790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComposeTransform:\n",
    "    \"\"\"Compose for (image, target) pairs.\"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "    \n",
    "class ToTensorTransform:\n",
    "    \"\"\"Convert PIL image to tensor, leave target unchanged\"\"\"\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image) # [C, H, W], float32 in [0, 1]\n",
    "        return image, target\n",
    "    \n",
    "class RandomHorizontalFlipTransform:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            # image: [1, H, W] or [C, H, W]\n",
    "            _, h, w = image.shape\n",
    "            image = torch.flip(image, dims=[2]) # flip width dimension\n",
    "\n",
    "            boxes = target[\"boxes\"]\n",
    "            if boxes.numel() > 0:\n",
    "            # boxes: [N, 4] in [x_min, y_min, x_max, y_max]\n",
    "                x_min = boxes[:, 0]\n",
    "                y_min = boxes[:, 1]\n",
    "                x_max = boxes[:, 2]\n",
    "                y_max = boxes[:, 3]\n",
    "\n",
    "                # flip x-coordinates: x' = w - x\n",
    "                new_x_min = w - x_max\n",
    "                new_x_max = w - x_min\n",
    "\n",
    "                boxes = torch.stack([new_x_min, y_min, new_x_max, y_max], dim=1)\n",
    "                target[\"boxes\"] = boxes\n",
    "\n",
    "        return image, target\n",
    "    \n",
    "class RandomBrightnessContrastTransform:\n",
    "    def __init__(self, brightness=0.1, contrast=0.1, p=0.5):\n",
    "        \"\"\"\n",
    "        Relative change of brightness and contrast.\n",
    "        brightness=0.1 means factor in [0.9, 1.1], etc. \n",
    "        \"\"\"\n",
    "        self.brightness = brightness\n",
    "        self.contrast = contrast\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            # image in tensor [C, H, W]\n",
    "            # Random brightness\n",
    "            if self.brightness > 0:\n",
    "                factor = 1.0 + random.uniform(-self.brightness, self.brightness)\n",
    "                image = F.adjust_brightness(image, factor)\n",
    "            # Random contrast\n",
    "            if self.contrast > 0:\n",
    "                factor = 1.0 + random.uniform(-self.contrast, self.contrast)\n",
    "                image = F.adjust_contrast(image, factor)\n",
    "            image = image.clamp(0.0, 1.0)\n",
    "        return image, target\n",
    "    \n",
    "class RandomGaussianNoiseTransform:\n",
    "    def __init__(self, sigma=0.01, p=0.5):\n",
    "        self.sigma = sigma\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            noise = torch.randn_like(image) * self.sigma\n",
    "            image = image + noise\n",
    "            image = image.clamp(0.0, 1.0)\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfdcc68",
   "metadata": {},
   "source": [
    "## Prepare DeepLesion dataset for DETR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b26fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for DeepLesion dataset\n",
    "class DeepLesionDataset(Dataset):\n",
    "    def __init__(self, root, split):\n",
    "        # Initialize dataset path, split and transformations\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "\n",
    "        # Apply data augmentations only for the train split\n",
    "        if split == \"train\":\n",
    "            self.transforms = ComposeTransform([\n",
    "                ToTensorTransform(),\n",
    "                RandomHorizontalFlipTransform(p=0.5),\n",
    "                RandomBrightnessContrastTransform(brightness=0.1, contrast=0.1, p=0.5),\n",
    "                RandomGaussianNoiseTransform(sigma=0.01, p=0.5),\n",
    "            ])\n",
    "        else:\n",
    "            self.transforms = ComposeTransform([\n",
    "                ToTensorTransform(), # Converts [0, 255] uint8 values to float [0.0, 1.0], and preserves 1 channel\n",
    "            ])\n",
    "\n",
    "        # Dataset logic (image paths, annotations, etc.)\n",
    "        self.image_dir = os.path.join(root, \"images\", split)\n",
    "        self.label_dir = os.path.join(root, \"labels\", split)\n",
    "        self.image_names = sorted([img for img in os.listdir(self.image_dir) if img.endswith(\".png\") or img.endswith(\".jpg\")])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_names[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        label_path = os.path.join(self.label_dir, os.path.splitext(image_name)[0] + \".txt\")\n",
    "\n",
    "        # 1) Load image and boxes in pixel coordinates\n",
    "        # Load grayscale PIL image\n",
    "        image = Image.open(image_path).convert(\"L\")\n",
    "\n",
    "        # Load corresponding bounding boxes and labels\n",
    "        boxes, labels = [], []\n",
    "        if os.path.exists(label_path):\n",
    "            for line in open(label_path):\n",
    "                cls, x_min, y_min, x_max, y_max = map(float, line.split())\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(int(cls) - 1) # Convert to 0...K-1 for DETR\n",
    "\n",
    "        # Create a target dictionary\n",
    "        if len(boxes) == 0:\n",
    "            boxes_t = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels_t = torch.zeros((0,), dtype=torch.int64)\n",
    "        else:\n",
    "            boxes_t = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels_t = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes_t,   # xyxy in pixels\n",
    "            \"labels\": labels_t\n",
    "        }\n",
    "\n",
    "        # 2) Apply transforms\n",
    "        if self.transforms:\n",
    "            image, target = self.transforms(image, target)\n",
    "        \n",
    "        # 3) Convert to normalized center-based format required by DETR\n",
    "        _, h_image, w_image = image.shape # image is now a Tensor, not PIL's Image object\n",
    "        boxes = target[\"boxes\"]\n",
    "        if len(boxes) > 0:\n",
    "            x_min, y_min, x_max, y_max = boxes.unbind(1)\n",
    "\n",
    "            # For DETR boxes must be center-based ([cx, cy, w, h])\n",
    "            bw = x_max - x_min\n",
    "            bh = y_max - y_min\n",
    "            cx = x_min + (bw / 2.0)\n",
    "            cy = y_min + (bh / 2.0)\n",
    "\n",
    "            # Normalize to [0, 1]\n",
    "            boxes = torch.stack([\n",
    "                cx / w_image,\n",
    "                cy / h_image,\n",
    "                bw / w_image,\n",
    "                bh / h_image\n",
    "            ], dim=1)\n",
    "\n",
    "        target[\"boxes\"] = boxes\n",
    "\n",
    "        return image, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc32327",
   "metadata": {},
   "source": [
    "## Prepare DataLoader objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da99fe0",
   "metadata": {},
   "source": [
    "### Set up the dataset's split path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf1673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [!] Same data splits as for Faster R-CNN\n",
    "deeplesion_detr_path = \"deeplesion_fasterrcnn_split_1\" # There are three splits: *_1, *_2 and *_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd2db9",
   "metadata": {},
   "source": [
    "### Set up the batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32192b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab\n",
    "train_batch_size = 4      # Set to 4 (keep 2-4, because that is a sweet spot for two-stage detectors. With higher values may hurt training dynamics)\n",
    "test_val_batch_size = 32   # Set to 32 (high value won't affect metric calculations, but increases memory usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf755a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local\n",
    "train_batch_size = 1\n",
    "test_val_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e96ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Shuffling is enabled for training DataLoader, because SGD benefits from seeing data in a new random order every epoch.\n",
    "  During validation and testing phases we do not need that, the order does not affect the metrics.\n",
    "\n",
    "- num_workers is the number of background processes that load & transorm batches in parallel. Good rule of thumb is num_workers being 2-4.\n",
    "\n",
    "- pin_memory, or pinned (page-locked) host memory, speeds up host to GPU copies and lets us use asynchronous transfers\n",
    "  It should be set to True if we train on GPU. It usually gives a small lbut real throughput bump. It consumes a bit more system RAM\n",
    "  and is useless on CPU-only runs.\n",
    "\n",
    "- Detection models expect lists of images and lists of target dicts, because each image can have different size and has a different\n",
    "  number of boxes. The default PyTorch collate tries to stack everything into tensors of the same shape, which breaks for \n",
    "  variable-length targets. Custom collate_fn function here unzips the list oof pairs into pair of lists so Faster R-CNN can consume them:\n",
    "    images: List[Tensor[C,H,W]]\n",
    "    targets: List[Dict{'boxes': Tensor[N,4], 'labels': Tensor[N]}]\n",
    "  That is exactly what torchvision's detection references use.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train_ds = DeepLesionDataset(deeplesion_data_dir / deeplesion_detr_path, \"train\")\n",
    "val_ds = DeepLesionDataset(deeplesion_data_dir / deeplesion_detr_path, \"val\")\n",
    "test_ds = DeepLesionDataset(deeplesion_data_dir / deeplesion_detr_path, \"test\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # This function turns a list of variable-sized images into\n",
    "    # a single padded tensor + mask. DETR does not handle padding\n",
    "    # internally.\n",
    "    # In short:\n",
    "    # - it pads images\n",
    "    # - it builds masks\n",
    "    # - it outputs ((images, masks), targets) \n",
    "    #\n",
    "    # batch: [(img1, target1), (img2, target2), ...]\n",
    "    # images: Tensor[B, C, H, W]\n",
    "    # masks: Tensor[B, H, W]\n",
    "    # target: dict with keys 'boxes' and 'labels'\n",
    "    # returns: ((images, masks), targets) \n",
    "    \n",
    "    images, targets = zip(*batch)\n",
    "\n",
    "    # Determine max size in batch\n",
    "    max_h = max(img.shape[1] for img in images)\n",
    "    max_w = max(img.shape[2] for img in images)\n",
    "    batch_size = len(images)\n",
    "    c = images[0].shape[0]\n",
    "\n",
    "    # Create padded batch tensor\n",
    "    batch_tensor = images[0].new_zeros((batch_size, c, max_h, max_w))\n",
    "\n",
    "    # Create mask (True = padding)\n",
    "    masks = torch.ones(\n",
    "        (batch_size, max_h, max_w), \n",
    "        dtype=torch.bool,\n",
    "        device=batch_tensor.device  # Create mask on the same device\n",
    "    )\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        _, h, w = img.shape\n",
    "        batch_tensor[i, :, :h, :w] = img\n",
    "        masks[i, :h, :w] = False\n",
    "\n",
    "    return (batch_tensor, masks), list(targets) \n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=test_val_batch_size, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=test_val_batch_size, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202e6f1d",
   "metadata": {},
   "source": [
    "# DETR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baabbdb",
   "metadata": {},
   "source": [
    "## Load and adjust the pre-trained DETR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df2ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "'''\n",
    "    - DETR uses set prediction with Hungarian matching\n",
    "    - The model explicitly predicts \"no object\"\n",
    "    - DETR uses a standard ResNet-50 backbone, whose first layer is: model.backbone.body.conv1\n",
    "'''\n",
    "#################################################################################################\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 8\n",
    "class_names = [\"bone\", \"abdomen\", \"mediastinum\", \"liver\", \"lung\", \"kidney\", \"soft_tissue\", \"pelvis\"]\n",
    "\n",
    "# Set up the available device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "def _make_detr_args(num_queries, num_classes, dilation):\n",
    "    \"\"\"\n",
    "    Method that reconstructs the args object used for creation of the DETR model.\n",
    "    \"\"\"\n",
    "\n",
    "    return SimpleNamespace(\n",
    "        # Training\n",
    "        lr=1e-4,\n",
    "        lr_backbone=1e-5,\n",
    "        batch_size=2,\n",
    "        weight_decay=1e-4,\n",
    "        epochs=100,\n",
    "        lr_drop=80,\n",
    "        clip_max_norm=0.1,\n",
    "\n",
    "        # Model\n",
    "        backbone=\"resnet50\",\n",
    "        dilation=dilation,\n",
    "        position_embedding=\"sine\",\n",
    "        enc_layers=6,\n",
    "        dec_layers=6,\n",
    "        dim_feedforward=2048,\n",
    "        hidden_dim=256,\n",
    "        dropout=0.1,\n",
    "        nheads=8,\n",
    "        num_queries=num_queries,\n",
    "        num_classes=num_classes,\n",
    "        pre_norm=False,\n",
    "\n",
    "        # Losses\n",
    "        aux_loss=True,\n",
    "        set_cost_class=1.0,\n",
    "        set_cost_bbox=5.0,\n",
    "        set_cost_giou=2.0,\n",
    "        class_loss_coef=1.0,\n",
    "        bbox_loss_coef=5.0,\n",
    "        giou_loss_coef=2.0,\n",
    "        eos_coef=0.1,\n",
    "\n",
    "        # Misc\n",
    "        masks=False,\n",
    "        device=device,\n",
    "        dataset_file=\"coco\",\n",
    "        coco_path=None,\n",
    "        remove_difficult=False,\n",
    "    )\n",
    "\n",
    "def _build_detr(args):\n",
    "    backbone = build_backbone(args)\n",
    "    transformer = build_transformer(args)\n",
    "\n",
    "    model = DETR(\n",
    "        backbone=backbone,\n",
    "        transformer=transformer,\n",
    "        num_classes=args.num_classes,\n",
    "        num_queries=args.num_queries,\n",
    "        aux_loss=True\n",
    "    )\n",
    "\n",
    "    matcher = HungarianMatcher(cost_class=args.set_cost_class, cost_bbox=args.set_cost_bbox, cost_giou=args.set_cost_giou)\n",
    "\n",
    "    weight_dict = {\n",
    "        \"loss_ce\": args.class_loss_coef,\n",
    "        \"loss_bbox\": args.bbox_loss_coef,\n",
    "        \"loss_giou\": args.giou_loss_coef\n",
    "    }\n",
    "\n",
    "    # aux loss handling\n",
    "    for i in range(transformer.decoder.num_layers - 1):\n",
    "        weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n",
    "\n",
    "    criterion = SetCriterion(\n",
    "        num_classes=num_classes,\n",
    "        matcher=matcher,\n",
    "        weight_dict=weight_dict,\n",
    "        eos_coef=args.eos_coef,\n",
    "        losses=[\"labels\", \"boxes\"]\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "    return model, criterion\n",
    "\n",
    "def construct_detr_model(num_queries, dilation=True):\n",
    "    \"\"\"\n",
    "    Method that constructs a DETR model using official build_model(),\n",
    "    with reduced num_queries and input adjusted to 1-channel.\n",
    "\n",
    "    If dilation=True, we replace stride with dilation in \n",
    "    the last convolutional block (DC5).\n",
    "    \"\"\"\n",
    "\n",
    "    # Build args\n",
    "    args = _make_detr_args(num_queries=num_queries, num_classes=num_classes, dilation=dilation)\n",
    "\n",
    "    # Build model\n",
    "    model, criterion = _build_detr(args)\n",
    "\n",
    "    # Load pretrained COCO weights\n",
    "    checkpoint = torch.hub.load_state_dict_from_url(\n",
    "        \"https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth\",\n",
    "        map_location=\"cpu\",\n",
    "        check_hash=True\n",
    "    )\n",
    "\n",
    "    state_dict = checkpoint[\"model\"]\n",
    "\n",
    "    # Remove incompatible COCO-specific weights parameters\n",
    "    del state_dict[\"query_embed.weight\"]\n",
    "    del state_dict[\"class_embed.weight\"]\n",
    "    del state_dict[\"class_embed.bias\"]\n",
    "\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    # Adapt DETR to 1-channel CT images\n",
    "    # - model is a Joiner(nn.Sequential)\n",
    "    # - model.backbone is a list of models where: \n",
    "    #   - backbone[0] -> CNN backbone (ResNet50)\n",
    "    #   - backbone[1] -> positional encoding model\n",
    "    old_conv = model.backbone[0].body.conv1 # shape: [64, 3, 7, 7] ( == [out_channels, in_channels, kernel_height, kernel_width])\n",
    "\n",
    "    new_conv = nn.Conv2d(\n",
    "        in_channels=1,\n",
    "        out_channels=old_conv.out_channels,\n",
    "        kernel_size=old_conv.kernel_size,\n",
    "        stride=old_conv.stride,\n",
    "        padding=old_conv.padding,\n",
    "        bias=False\n",
    "    )\n",
    "\n",
    "    # Initialize 1-channel conv layer with averaged RGB weights (CT-appropriate)\n",
    "    with torch.no_grad():\n",
    "        new_conv.weight[:] = old_conv.weight.mean(dim=1, keepdim=True)\n",
    "\n",
    "    model.backbone[0].body.conv1 = new_conv\n",
    "\n",
    "    # [OPTIONAL] Freeze 2 first backbone layers\n",
    "    # for name, param in model.backbone[0].named_parameters():\n",
    "    #     if \"layer1\" in name or \"layer2\" in name:\n",
    "    #         param.requires_grad = False\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "    # Sanity check\n",
    "    # print(f\"First conv layer shape: {model.backbone[0].body.conv1.weight.shape}\") # Should be [64, 1, 7, 7]\n",
    "\n",
    "    return model, criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4af3f6",
   "metadata": {},
   "source": [
    "## Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998ce540",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- For DETR it is common to use Adam as the optimizer.\n",
    "- Hyperparameters:\n",
    "    - momentum: \n",
    "        adds an exponential moving average of past gradients to the current step, which causes smoother updates,\n",
    "        less zig-zagging and faster convergance. Typically set to 0.9, and rarely needs tuning.\n",
    "    - weight_decay (L2 regularization):\n",
    "        Penalizes large weights to reduce overfitting (shrinks params each step).\n",
    "    - step_size (in StepLR):\n",
    "        Every step_size epochs, the LR scheduler triggers a decay.\n",
    "    - gamma (in StepLR):\n",
    "        Multiplicative LR factor at each step: new_lr = old_lr * gamma. Commonly set to 0.1.\n",
    "\n",
    "- Cross-validate empirically, due to computation power constraints.\n",
    "\n",
    "- optimizer.zero_grad(set_to_none=True) - set_to_none parameter set to True means that for each parameter param.grad is set to None\n",
    "  (no tensor is kept). On the next backward() PyTorch allocates a fresh grad tensor and writes into it. It causes faster & less memory traffic\n",
    "  by avoiding writing zeros over large grad buffers every step. Lowers memory footprint by letting unused grads be garbage-collected and reallocated\n",
    "  only when needed.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def train_one_config(\n",
    "    train_loader, val_loader, device,\n",
    "    learning_rate=1e-4, weight_decay=1e-4,\n",
    "    max_epochs=100, patience=30, metric_key=\"mAP50_95\",\n",
    "    gamma=0.1, warmup_epochs=1, num_queries=50, dilation=True, step_size=40\n",
    "):\n",
    "    # 1) Construct the model\n",
    "    model, criterion = construct_detr_model(num_queries=num_queries, dilation=dilation)\n",
    "\n",
    "    # 2) Set up optimizer with param groups (different LRs for head, transformer and backbone)\n",
    "    backbone_lr = learning_rate * 0.1\n",
    "    param_dicts = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if n.startswith(\"backbone.\") and p.requires_grad],\n",
    "            \"lr\": backbone_lr,\n",
    "            \"initial_lr\": backbone_lr,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if (not n.startswith(\"backbone.\")) and p.requires_grad],\n",
    "            \"lr\": learning_rate,\n",
    "            \"initial_lr\": learning_rate,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(param_dicts, weight_decay=weight_decay)\n",
    "\n",
    "    # 3) Learning rate scheduler (CosineAnnealingLR is fine, but StepLR is also common for DETR)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[70], gamma=gamma)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs, eta_min=0.0)\n",
    "\n",
    "    best_metric = -float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    best_model = None\n",
    "    best_optimizer = None\n",
    "    best_scheduler = None\n",
    "    epochs_no_improve = 0\n",
    "    history = []\n",
    "    global_step = 0\n",
    "\n",
    "    # Set up warmup\n",
    "    warmup_iters = warmup_epochs * len(train_loader)\n",
    "\n",
    "    # 4) Train model\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        print(f\"\\n*** Epoch [{epoch}/{max_epochs}] started ***\")\n",
    "        model.train()\n",
    "        criterion.train()\n",
    "        running_loss = 0.0\n",
    "        n_processed_images = 0\n",
    "\n",
    "        # -- Training loop\n",
    "        for batch_idx, batch in enumerate(train_loader, start=1):\n",
    "            global_step += 1\n",
    "            \n",
    "            # ---- Warmup phase\n",
    "            if global_step <= warmup_iters:\n",
    "                warmup_factor = global_step / float(warmup_iters)\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group[\"lr\"] = param_group[\"initial_lr\"] * warmup_factor\n",
    "                warmup_percent = 100.0 * warmup_factor\n",
    "            else: # let scheduler manage LR\n",
    "                warmup_percent = 100.0\n",
    "\n",
    "            # ---- Unpack DETR batch\n",
    "            (images, masks), targets = batch\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            targets = [{key: val.to(device) for key, val in target.items()} for target in targets]\n",
    "\n",
    "            # ---- Wrap into NestedTensor (facebook DETR expects this)\n",
    "            nested_tensor = NestedTensor(images, masks)\n",
    "            \n",
    "            # ---- Forward + loss\n",
    "            outputs = model(nested_tensor)\n",
    "            loss_dict = criterion(outputs, targets)\n",
    "            weight_dict = criterion.weight_dict\n",
    "            loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict if k in weight_dict)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True) # Allocates grads fresh during backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += float(loss.item())\n",
    "            n_processed_images += images.size(0)\n",
    "\n",
    "            if batch_idx % 10 == 0 or batch_idx == len(train_loader):\n",
    "                avg_loss_so_far = running_loss / batch_idx\n",
    "                status = (\n",
    "                    f\"\\rEpoch: [{epoch}/{max_epochs}], \"\n",
    "                    f\"Images: [{n_processed_images}/{len(train_loader.dataset)}], \"\n",
    "                    f\"Warmup: [{float(warmup_percent):.2f}%], \"\n",
    "                    f\"Loss: {avg_loss_so_far:.4f}\"\n",
    "                )\n",
    "                print(status, end=\"\", flush=True)\n",
    "\n",
    "        print()\n",
    "        lr_scheduler.step()\n",
    "        train_loss = running_loss / max(1, len(train_loader))\n",
    "        print(f\"*** Epoch [{epoch}/{max_epochs}] finished -> Loss: {train_loss:.4f} ***\")\n",
    "\n",
    "        # Validation\n",
    "        print(\"*** Validation started ***\")\n",
    "        val_metrics = evaluate_detector(model, val_loader, device, num_classes, class_names)\n",
    "        val_score = float(val_metrics[metric_key])\n",
    "\n",
    "        history.append({\"epoch\": epoch, \"train_loss\": train_loss, **val_metrics})\n",
    "        print(f\"Loss={train_loss:.4f}, mAP50={val_metrics['mAP50']:.4f}, mAP50_95={val_metrics['mAP50_95']:.4f}\")\n",
    "        print(f\"*** Validation finished ***\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_score > best_metric + 1e-6:\n",
    "            best_metric = val_score\n",
    "            best_epoch = epoch\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            best_optimizer = copy.deepcopy(optimizer.state_dict())\n",
    "            best_scheduler = copy.deepcopy(lr_scheduler.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"*** Early stopping after {epoch}/{max_epochs} epochs (best at {best_epoch} with {metric_key}={best_metric:.4f}). ***\")\n",
    "                break\n",
    "\n",
    "    return {\n",
    "        \"best_metric\": best_metric,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"best_model\": best_model,\n",
    "        \"best_optimizer\": best_optimizer,\n",
    "        \"best_scheduler\": best_scheduler,\n",
    "        \"history\": history,\n",
    "        \"lr\": learning_rate,\n",
    "        \"weight_decay\": weight_decay\n",
    "    }\n",
    "\n",
    "# =================================================================================================================================================\n",
    "# =================================================================================================================================================\n",
    "\n",
    "def plot_training_history(result, title=None):\n",
    "    \"\"\"\n",
    "    Plots the history of one training configuration. \n",
    "    result is the dictionary returned by train_one_config\n",
    "    \"\"\"\n",
    "\n",
    "    history = result[\"history\"]\n",
    "    epochs = [ h[\"epoch\"] for h in history ]\n",
    "    train_losses = [ h[\"train_loss\"] for h in history ]\n",
    "    map50 = [ h[\"mAP50\"] for h in history ]\n",
    "    map50_95 = [ h[\"mAP50_95\"] for h in history ]\n",
    "\n",
    "    if title is None:\n",
    "        title = f\"lr={result['lr']}, wd={result['weight_decay']}\"\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "    # Loss on left y-axis\n",
    "    ax1.plot(epochs, train_losses, color=\"blue\", marker=\"o\", label=\"Train loss\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.grid(True, which=\"both\", axis=\"both\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "    # mAP on right y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(epochs, map50, color=\"green\", marker=\"x\", linestyle=\"-\", label=\"mAP50\")\n",
    "    ax2.plot(epochs, map50_95, color=\"red\", marker=\"s\", linestyle=\":\", label=\"mAP50-95\")\n",
    "    ax2.set_ylabel(\"mAP\")\n",
    "\n",
    "    # Combine legends\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"best\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =================================================================================================================================================\n",
    "# =================================================================================================================================================\n",
    "\n",
    "# Set up hyperparameters\n",
    "learning_rates = [1e-4] # 1e-4 for DETR\n",
    "weight_decays = [1e-4] # [1e-4, 5e-4]\n",
    "max_epochs = 100\n",
    "num_queries = 50\n",
    "dilation = True\n",
    "warmup_epochs = 1\n",
    "patience = max_epochs + 1\n",
    "best_result = None\n",
    "\n",
    "print(\"*** Training started ***\")\n",
    "for learning_rate in learning_rates:\n",
    "    for weight_decay in weight_decays:\n",
    "        print(f\"[HYPERPARAMETERS]:\\n\"\n",
    "              f\"    learning_rate: {learning_rate}\\n\"\n",
    "              f\"    weight_decay: {weight_decay}\\n\"\n",
    "              f\"    num_queries: {num_queries}\\n\"\n",
    "              f\"    dilation: {dilation}\\n\")\n",
    "        result = train_one_config(\n",
    "            train_loader=train_loader, val_loader=val_loader, device=device,\n",
    "            learning_rate=learning_rate, weight_decay=weight_decay, max_epochs=max_epochs, patience=patience,\n",
    "            metric_key=\"mAP50_95\", warmup_epochs=warmup_epochs, num_queries=num_queries, dilation=dilation\n",
    "        )\n",
    "\n",
    "        plot_training_history(result)\n",
    "\n",
    "        if best_result is None or result[\"best_metric\"] > best_result[\"best_metric\"]:\n",
    "            best_result = result\n",
    "\n",
    "plot_training_history(best_result, title=f\"[BEST] lr={result['lr']}, wd={result['weight_decay']}\")\n",
    "print(\"*** Training complete ***\")\n",
    "print(f\"Best config: lr={best_result['lr']} wd={best_result['weight_decay']} \"\n",
    "      f\"epoch={best_result['best_epoch']} mAP50_95={best_result['best_metric']:.4f}\")\n",
    "\n",
    "# Save the best model checkpoint\n",
    "best_checkpoint = {\n",
    "    \"state_dict\": best_result[\"best_model\"],\n",
    "    \"best_optimizer\": best_result[\"best_optimizer\"],\n",
    "    \"best_scheduler\": best_result[\"best_scheduler\"],\n",
    "    \"epoch\": best_result[\"best_epoch\"],\n",
    "    \"metric_key\": \"mAP50_95\",\n",
    "    \"metric_value\": best_result[\"best_metric\"],\n",
    "    \"hp\": {\n",
    "        \"lr\": best_result[\"lr\"],\n",
    "        \"weight_decay\": best_result[\"weight_decay\"],\n",
    "        \"momentum\": 0.9,\n",
    "        \"step_size\": 3,\n",
    "        \"gamma\": 0.1,\n",
    "        \"max_epochs\": max_epochs,\n",
    "        \"patience\": patience,\n",
    "    },\n",
    "    # Describe how to reconstruct the model\n",
    "    \"model_spec\": {\n",
    "        \"arch\": \"detr_resnet50\",\n",
    "        \"min_size\": 512,\n",
    "        \"max_size\": 512,\n",
    "        \"in_channels\": 1,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"num_queries\": num_queries,\n",
    "        \"dilation\": dilation,\n",
    "        \"image_mean\": [0.5],\n",
    "        \"image_std\": [0.5],\n",
    "    },\n",
    "    \"class_names\": class_names,\n",
    "    \"versions\": {\"torch\": torch.__version__, \"torchvision\": torchvision.__version__},\n",
    "}\n",
    "\n",
    "# Save checkpoint locally\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "save_path = f\"checkpoints/detr_best_{ts}.pt\"\n",
    "torch.save(best_checkpoint, save_path)\n",
    "print(f\"[{ts}] Saved best checkpoint to {save_path}\")\n",
    "\n",
    "# [GOOGLE COLLAB ONLY - uncomment] Save checkpoint on Google drive\n",
    "# gdrive_save_dir = Path(\"detr_checkpoints\")\n",
    "# gdrive_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "# gdrive_save_path = os.path.join(str(gdrive_save_dir), f\"detr_best_{ts}.pt\")\n",
    "# torch.save(best_checkpoint, gdrive_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bccbb",
   "metadata": {},
   "source": [
    "## Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e1d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best weights and evaluate on the test set\n",
    "best_checkpoint = torch.load(save_path, map_location=\"cpu\", weights_only=False)\n",
    "best_model, _ = construct_detr_model(\n",
    "    num_queries=best_checkpoint[\"model_spec\"][\"num_queries\"],\n",
    "    dilation=best_checkpoint[\"model_spec\"][\"dilation\"]\n",
    ")\n",
    "best_model.load_state_dict(best_checkpoint[\"state_dict\"])\n",
    "\n",
    "# Evaluate the best model\n",
    "print(f\"*** Evaluation started ***\")\n",
    "test_metrics = evaluate_detector(best_model, test_loader, device, num_classes, class_names)\n",
    "\n",
    "print(f\"*** Evaluation finished ***\")\n",
    "print(test_metrics)\n",
    "print_result_report(test_metrics, test_loader, class_names)\n",
    "print_froc_curve_info(best_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef06730",
   "metadata": {},
   "source": [
    "# Disconnect Google Colab runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1a80d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lesion_detector_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
