{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad82646",
   "metadata": {},
   "source": [
    "# Supervised Lesion Detector\n",
    "\n",
    "This notebook is dedicated to training and evaluating a supervised lesion detector on DeepLesion dataset with the following supervised model architectures for image detection with ResNet-50 backbone:\n",
    "- YOLOv5 (stable, but YOLOv8 is newer),\n",
    "- Faster R-CNN (torchvision.models.detection or Detectron2),\n",
    "- DETR (Facebook DETR),\n",
    "- Improved DETR (DINO-DETR or Deformable DETR, DINO has better performance but Deformable is faster),\n",
    "- RetinaNet (FPN backbone + anchor-based).\n",
    "\n",
    "## Assumptions:\n",
    "- Use 2D slice inputs (optionally use the neighbouring ones too),\n",
    "- Resize all images to 512x512,\n",
    "- Use COCO-style Dataset class.\n",
    "- Use DeepLesion for training a general lesion localizer and some other like LiTS (Liver Tumor Segmentation) or CHAOS (CT liver dataset) for more specialized localizer.\n",
    "\n",
    "## ðŸ“š Thesis Value Summary\n",
    "### Contribution and Value:\n",
    "- Comparison of CNN vs Transformer detectors on DeepLesion\t-> âœ… Fills a gap in literature\n",
    "- Evaluation of improved DETRs (DINO/Deformable) -> âœ… Modern insight\n",
    "- General vs specialized lesion detection -> âœ… Strong clinical relevance\n",
    "- Analysis of training time, robustness, failure modes -> âœ… Engineering depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac67d88",
   "metadata": {},
   "source": [
    "# Google Colab only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b5a33",
   "metadata": {},
   "source": [
    "### Download required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8b79fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/pmalesa/lesion_detector/main/notebooks/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf8a0f5",
   "metadata": {},
   "source": [
    "### Mount DeepLesion images from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e80df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content\n",
    "!rm -rf data/deeplesion   # remove existing link if any\n",
    "!mkdir -p data\n",
    "!ln -s /content/drive/MyDrive/deeplesion/data/deeplesion data/deeplesion\n",
    "!ls -l data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1901fb48",
   "metadata": {},
   "source": [
    "# Import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "715d9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.typing import NDArray\n",
    "from typing import Any\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "# Faster R-CNN packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import torchvision.ops as ops\n",
    "import copy\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc32778b",
   "metadata": {},
   "source": [
    "# Image preprocessing and utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3afee5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata(path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads metadata from the given path and\n",
    "    returns it as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def normalize(img: NDArray[np.uint16], per_image_norm: bool):\n",
    "    \"\"\"\n",
    "    Normalizes the input image\n",
    "    \"\"\"\n",
    "\n",
    "    img = img.astype(np.float32)\n",
    "    if not per_image_norm:\n",
    "        return img / 65535.0\n",
    "    max = np.max(img)\n",
    "    min = np.min(img)\n",
    "    img = (img - min) / (max - min)\n",
    "    return img\n",
    "\n",
    "def convert_to_hu(img: NDArray[np.uint16], norm: bool, hu_min=-1024, hu_max=3071):\n",
    "    \"\"\"\n",
    "    Converts the pixel data of a uint16\n",
    "    CT image to Hounsfield Units (HU).\n",
    "    \"\"\"\n",
    "\n",
    "    hu_img = img.astype(np.int32) - 32768\n",
    "    hu_img = np.clip(hu_img, hu_min, hu_max).astype(np.float32)\n",
    "    if norm:\n",
    "        hu_img = (hu_img - hu_min) / (hu_max - hu_min)\n",
    "        hu_img = np.clip(hu_img, 0.0, 1.0)\n",
    "    return hu_img\n",
    "\n",
    "def load_image(path: str, hu_scale: bool = True, norm: bool = True, per_image_norm: bool = True):\n",
    "    \"\"\"\n",
    "    Loads an image given its path\n",
    "    and returns it as a numpy array.\n",
    "    \"\"\"\n",
    "    \n",
    "    img = Image.open(path)\n",
    "    img_array = np.array(img)\n",
    "    if hu_scale:\n",
    "        hu_min = -160\n",
    "        hu_max = 240\n",
    "        return convert_to_hu(img_array, norm, hu_min, hu_max)\n",
    "    elif norm:\n",
    "        return normalize(img_array, per_image_norm)\n",
    "    return img_array\n",
    "\n",
    "def save_image(img_array: NDArray[Any], path: str):\n",
    "    \"\"\"\n",
    "    Saves the input image to the specified path.\n",
    "    \"\"\"\n",
    "\n",
    "    if img_array.dtype != np.uint16:\n",
    "        img_array = img_array.astype(np.uint16)\n",
    "    img = Image.fromarray(img_array)\n",
    "    img.save(path)\n",
    "\n",
    "def save_normalized_image_uint8(img_array: NDArray[Any], path: str):\n",
    "    \"\"\"\n",
    "    Saves the normalized input image data (0.0 - 1.0) \n",
    "    to the specified path with uint8 precision (0 - 255).\n",
    "    \"\"\"\n",
    "\n",
    "    img_array_scaled = (img_array * 255.0).clip(0, 255).astype(np.uint8)\n",
    "    img = Image.fromarray(img_array_scaled)\n",
    "    img.save(path)\n",
    "\n",
    "def show_image(img: NDArray[np.float32], title=\"Example Image\", cmap=\"gray\"):\n",
    "    \"\"\"\n",
    "    Shows the image given its data, title and colour map.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e661233b",
   "metadata": {},
   "source": [
    "# Set paths to DeepLesion images and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b15776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to unprocessed data\n",
    "# [!] In Google Colab change to: \"data/deeplesion/deeplesion_metadata.csv\" and \"data/deeplesion/key_slices/\" respectively [!]\n",
    "deeplesion_metadata_path = Path(\"../data/deeplesion_metadata.csv\")\n",
    "deeplesion_image_path = Path(\"../data/deeplesion/key_slices/\")\n",
    "\n",
    "# Paths to processed data\n",
    "deeplesion_data_dir = Path(\"data/deeplesion/\")\n",
    "deeplesion_preprocessed_image_path = deeplesion_data_dir / \"deeplesion_preprocessed_uint8/key_slices\"\n",
    "deeplesion_preprocessed_metadata_path = deeplesion_data_dir / \"deeplesion_preprocessed_uint8/deeplesion_metadata_preprocessed.csv\"\n",
    "\n",
    "# Path to deeplesion metadata in COCO format\n",
    "deeplesion_coco_json_path = deeplesion_data_dir / \"deeplesion_coco.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886b3ffe",
   "metadata": {},
   "source": [
    "# Preprocess the DeepLesion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37851b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preprocess the DeepLesion dataset by:\n",
    "    - Choosing only images that come from the original validation and test set divisions (only these have lesion type annotations),\n",
    "    - Normalizing each image according to a fixed HU scale window [-160, 240] (can be also [-1024, 3071]),\n",
    "    - Resizing each image to 512x512 if necessary (and adjusting the target bounding box coordinates),\n",
    "    - Saving the new metadata file with resized images and adjusted target bounding boxes. \n",
    "'''\n",
    "\n",
    "deeplesion_metadata = load_metadata(deeplesion_metadata_path)\n",
    "\n",
    "# Clear the existing directory with preprocessed images\n",
    "if deeplesion_preprocessed_image_path.exists() and deeplesion_preprocessed_image_path.is_dir():\n",
    "    shutil.rmtree(deeplesion_preprocessed_image_path)\n",
    "deeplesion_preprocessed_image_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for idx, row in deeplesion_metadata.iterrows():\n",
    "    # Extract only images with annotated lesions (Val + Test)\n",
    "    if row[\"Train_Val_Test\"] == 1:\n",
    "        continue\n",
    "\n",
    "    file_name = row[\"File_name\"]\n",
    "    bbox_str = row[\"Bounding_boxes\"]\n",
    "    size_str = row[\"Image_size\"]\n",
    "    image_path = os.path.join(deeplesion_image_path, file_name)\n",
    "    preprocessed_image_path = os.path.join(deeplesion_preprocessed_image_path, file_name)\n",
    "    image_data = load_image(image_path)\n",
    "\n",
    "    # Extract ground truth bounding box' coordinates\n",
    "    bbox_coords = [float(val) for val in bbox_str.split(\",\")]\n",
    "    x1, y1, x2, y2 = [round(c) for c in bbox_coords]\n",
    "\n",
    "    # Extract sizes\n",
    "    image_sizes = [int(val) for val in size_str.split(\",\")]\n",
    "    width, height = [size for size in image_sizes]\n",
    "\n",
    "    # Rescale to (512 x 512) if necessary\n",
    "    if (width, height) != (512, 512):\n",
    "        image_data = cv2.resize(\n",
    "            image_data, (512, 512), interpolation=cv2.INTER_AREA\n",
    "        )\n",
    "        scale_x = 512 / width\n",
    "        scale_y = 512 / height\n",
    "        x1 = round(x1 * scale_x)\n",
    "        y1 = round(y1 * scale_y)\n",
    "        x2 = round(x2 * scale_x)\n",
    "        y2 = round(y2 * scale_y)\n",
    "        height = 512\n",
    "        width = 512\n",
    "\n",
    "        deeplesion_metadata.at[idx, \"Bounding_boxes\"] = f\"{x1}, {y1}, {x2}, {y2}\"\n",
    "        deeplesion_metadata.at[idx, \"Image_size\"] = \"512, 512\"\n",
    "\n",
    "    save_normalized_image_uint8(image_data, preprocessed_image_path)    \n",
    "\n",
    "# Save preprocessed metadata csv file\n",
    "deeplesion_metadata.to_csv(deeplesion_preprocessed_metadata_path)\n",
    "\n",
    "# Verify if all images have 512x512 size\n",
    "deeplesion_metadata_preprocessed = load_metadata(deeplesion_preprocessed_metadata_path)\n",
    "for idx, row in deeplesion_metadata_preprocessed.iterrows():\n",
    "    if row[\"Train_Val_Test\"] == 1:\n",
    "        continue\n",
    "    size_str = row[\"Image_size\"]\n",
    "    image_sizes = [int(val) for val in size_str.split(\",\")]\n",
    "    width, height = [size for size in image_sizes]\n",
    "    if (width, height) != (512, 512):\n",
    "        raise ValueError(f\"ERROR: Not all images have required size!\")\n",
    "print(f\"SUCCES: All images were preprocessed correctly.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc4fda",
   "metadata": {},
   "source": [
    "# Convert DeepLesion metadata to COCO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplesion_metadata_preprocessed = load_metadata(deeplesion_preprocessed_metadata_path)\n",
    "\n",
    "images = []\n",
    "annotations = []\n",
    "categories = [\n",
    "    {\"id\": 1, \"name\": \"bone\"},\n",
    "    {\"id\": 2, \"name\": \"abdomen\"},\n",
    "    {\"id\": 3, \"name\": \"mediastinum\"},\n",
    "    {\"id\": 4, \"name\": \"liver\"},\n",
    "    {\"id\": 5, \"name\": \"lung\"},\n",
    "    {\"id\": 6, \"name\": \"kidney\"},\n",
    "    {\"id\": 7, \"name\": \"soft tissue\"},\n",
    "    {\"id\": 8, \"name\": \"pelvis\"}\n",
    "]\n",
    "\n",
    "image_counter = 1\n",
    "annotation_id = 1\n",
    "image_id_map = {}\n",
    "\n",
    "for idx, row in deeplesion_metadata_preprocessed.iterrows():\n",
    "    # Extract only images with annotated lesions (Val + Test)\n",
    "    if row[\"Train_Val_Test\"] == 1:\n",
    "        continue\n",
    "\n",
    "    file_name = row[\"File_name\"]\n",
    "    lesion_type = row[\"Coarse_lesion_type\"]\n",
    "    bbox_str = row[\"Bounding_boxes\"]\n",
    "    size_str = row[\"Image_size\"]\n",
    "\n",
    "    # Extract ground truth bounding box' coordinates\n",
    "    bbox_coords = [float(val) for val in bbox_str.split(\",\")]\n",
    "    x1, y1, x2, y2 = [round(c) for c in bbox_coords]\n",
    "\n",
    "    # Extract sizes\n",
    "    image_sizes = [int(val) for val in size_str.split(\",\")]\n",
    "    width, height = [size for size in image_sizes]\n",
    "\n",
    "    # Register image\n",
    "    if file_name not in image_id_map:\n",
    "        image_id_map[file_name] = image_counter\n",
    "        images.append({\n",
    "            \"id\": image_counter,\n",
    "            \"file_name\": file_name,\n",
    "            \"width\": width,\n",
    "            \"height\": height\n",
    "        })\n",
    "        image_counter += 1\n",
    "    image_id = image_id_map[file_name]\n",
    "\n",
    "    # Initialize ground truth bounding box' parameters\n",
    "    bbox = [x1, y1, x2 - x1, y2 - y1]\n",
    "    area = bbox[2] * bbox[3]\n",
    "\n",
    "    annotations.append({\n",
    "        \"id\": annotation_id,\n",
    "        \"image_id\": image_id,\n",
    "        \"category\": lesion_type,\n",
    "        \"bbox\": bbox,\n",
    "        \"area\": area,\n",
    "        \"iscrowd\": 0    # Normal object (not crowd of indistinct objects, that can't be cleanly separated)\n",
    "    })\n",
    "\n",
    "    annotation_id += 1\n",
    "\n",
    "# Save to JSON\n",
    "deeplesion_coco_format = {\n",
    "    \"images\": images,\n",
    "    \"annotations\": annotations,\n",
    "    \"categories\": categories\n",
    "}\n",
    "\n",
    "with open(deeplesion_coco_json_path, \"w\") as f:\n",
    "    json.dump(deeplesion_coco_format, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac5c7c6",
   "metadata": {},
   "source": [
    "# Convert DeepLesion dataset to YOLOv5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e14665",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplesion_metadata_preprocessed = load_metadata(deeplesion_preprocessed_metadata_path)\n",
    "\n",
    "# Source directories\n",
    "image_dir = deeplesion_preprocessed_image_path\n",
    "label_dir = deeplesion_data_dir / \"labels_unsorted\"\n",
    "\n",
    "# Create .txt files\n",
    "label_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create target directory\n",
    "target_dir = deeplesion_data_dir / \"deeplesion_yolo\"\n",
    "if target_dir.exists() and target_dir.is_dir():\n",
    "    shutil.rmtree(target_dir)\n",
    "\n",
    "for idx, row in deeplesion_metadata_preprocessed.iterrows():\n",
    "    # Extract only images with annotated lesions (Val + Test)\n",
    "    if row[\"Train_Val_Test\"] == 1:\n",
    "        continue\n",
    "\n",
    "    file_name = row[\"File_name\"]\n",
    "    image_path = os.path.join(str(image_dir), file_name)\n",
    "    label_path = os.path.join(str(label_dir), file_name.replace(\".png\", \".txt\"))\n",
    "\n",
    "    if not os.path.exists(image_path):\n",
    "        continue\n",
    "\n",
    "    lesion_type = row[\"Coarse_lesion_type\"] - 1 # YOLOv5 requires class IDs starting at 0\n",
    "    bbox_str = row[\"Bounding_boxes\"]\n",
    "    size_str = row[\"Image_size\"]\n",
    "\n",
    "    # Extract ground truth bounding box' coordinates\n",
    "    bbox_coords = [float(val) for val in bbox_str.split(\",\")]\n",
    "    x1, y1, x2, y2 = [round(c) for c in bbox_coords]\n",
    "\n",
    "    # Extract sizes\n",
    "    image_sizes = [int(val) for val in size_str.split(\",\")]\n",
    "    width, height = [size for size in image_sizes]\n",
    "\n",
    "    bbox_width = x2 - x1\n",
    "    bbox_height = y2 - y1\n",
    "    x_center = x1 + bbox_width / 2\n",
    "    y_center = y1 + bbox_height / 2\n",
    "\n",
    "    # Normalize\n",
    "    x_center /= width\n",
    "    y_center /= height\n",
    "    bbox_width /= width\n",
    "    bbox_height /= height\n",
    "\n",
    "    with open(label_path, 'a') as f:\n",
    "        f.write(f\"{lesion_type} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "for split in splits:\n",
    "    (target_dir / \"images\" / split).mkdir(parents=True, exist_ok=True)\n",
    "    (target_dir / \"labels\" / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Collect all images with annotated lesions\n",
    "annotated_images = [img for img in image_dir.glob(\"*.png\") if (label_dir / (img.stem + \".txt\")).exists()]\n",
    "random.seed(42) # TODO - Change this seed to create different train/val/test splits (42, 314, 666)\n",
    "random.shuffle(annotated_images)\n",
    "\n",
    "# Split into train, val and test sets\n",
    "n_total = len(annotated_images)\n",
    "n_train = int(0.7 * n_total)\n",
    "n_val = int(0.15 * n_total)\n",
    "\n",
    "train_images = annotated_images[:n_train]\n",
    "val_images = annotated_images[n_train:n_train + n_val]\n",
    "test_images = annotated_images[n_train + n_val:]\n",
    "\n",
    "splits_map = {\n",
    "    \"train\": train_images,\n",
    "    \"val\": val_images,\n",
    "    \"test\": test_images\n",
    "}\n",
    "\n",
    "# Copy image files\n",
    "for split, images in splits_map.items():\n",
    "    for image_path in images:\n",
    "        label_path = label_dir / (image_path.stem + \".txt\")\n",
    "        shutil.copy(image_path, target_dir / \"images\" / split / image_path.name)\n",
    "        shutil.copy(label_path, target_dir / \"labels\" / split / label_path.name)\n",
    "\n",
    "print(f\"Split done! Total = {n_total}\")\n",
    "\n",
    "# Generate deeplesion.yaml\n",
    "dataset_root = os.path.abspath(str(target_dir))\n",
    "deeplesion_yaml = {\n",
    "    \"path\": dataset_root,\n",
    "    \"train\": os.path.join(dataset_root, \"images/train\"),\n",
    "    \"val\": os.path.join(dataset_root, \"images/val\"),\n",
    "    \"test\": os.path.join(dataset_root, \"images/test\"),\n",
    "    \"nc\": 8,\n",
    "    \"names\": [\n",
    "        \"bone\",\n",
    "        \"abdomen\",\n",
    "        \"mediastinum\",\n",
    "        \"liver\",\n",
    "        \"lung\",\n",
    "        \"kidney\",\n",
    "        \"soft_tissue\",\n",
    "        \"pelvis\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(target_dir / \"deeplesion.yaml\", \"w\") as f:\n",
    "    yaml.dump(deeplesion_yaml, f)\n",
    "\n",
    "# Remove directory with unsorted labels\n",
    "if label_dir.exists() and label_dir.is_dir():\n",
    "    shutil.rmtree(label_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b96b3f",
   "metadata": {},
   "source": [
    "# Convert DeepLesion dataset to Faster R-CNN format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d7f226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split done! Total = 9624\n"
     ]
    }
   ],
   "source": [
    "deeplesion_metadata_preprocessed = load_metadata(deeplesion_preprocessed_metadata_path)\n",
    "\n",
    "# Source directories\n",
    "image_dir = deeplesion_preprocessed_image_path\n",
    "label_dir = deeplesion_data_dir / \"labels_unsorted\"\n",
    "\n",
    "# Create .txt files\n",
    "label_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create target directory\n",
    "target_dir = deeplesion_data_dir / \"deeplesion_fasterrcnn_split_X\"\n",
    "if target_dir.exists() and target_dir.is_dir():\n",
    "    shutil.rmtree(target_dir)\n",
    "\n",
    "for idx, row in deeplesion_metadata_preprocessed.iterrows():\n",
    "    # Extract only images with annotated lesions (Val + Test)\n",
    "    if row[\"Train_Val_Test\"] == 1:\n",
    "        continue\n",
    "\n",
    "    file_name = row[\"File_name\"]\n",
    "    image_path = os.path.join(str(image_dir), file_name)\n",
    "    label_path = os.path.join(str(label_dir), file_name.replace(\".png\", \".txt\"))\n",
    "\n",
    "    if not os.path.exists(image_path):\n",
    "        continue\n",
    "\n",
    "    lesion_type = row[\"Coarse_lesion_type\"] # Faster R-CNN uses the class 0 implicitly as the background class (no need for subtraction)\n",
    "    bbox_str = row[\"Bounding_boxes\"]\n",
    "    size_str = row[\"Image_size\"]\n",
    "\n",
    "    # Extract ground truth bounding box' coordinates\n",
    "    bbox_coords = [float(val) for val in bbox_str.split(\",\")]\n",
    "    x1, y1, x2, y2 = [round(c) for c in bbox_coords]\n",
    "\n",
    "    with open(label_path, 'a') as f:\n",
    "        f.write(f\"{lesion_type} {x1} {y1} {x2} {y2}\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "for split in splits:\n",
    "    (target_dir / \"images\" / split).mkdir(parents=True, exist_ok=True)\n",
    "    (target_dir / \"labels\" / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Collect all images with annotated lesions\n",
    "annotated_images = [img for img in image_dir.glob(\"*.png\") if (label_dir / (img.stem + \".txt\")).exists()]\n",
    "random.seed(42) # TODO - Change this seed to create different train/val/test splits (42, 314, 666)\n",
    "random.shuffle(annotated_images)\n",
    "\n",
    "# Split into train, val and test sets\n",
    "n_total = len(annotated_images)\n",
    "n_train = int(0.7 * n_total)\n",
    "n_val = int(0.15 * n_total)\n",
    "\n",
    "train_images = annotated_images[:n_train]\n",
    "val_images = annotated_images[n_train:n_train + n_val]\n",
    "test_images = annotated_images[n_train + n_val:]\n",
    "\n",
    "splits_map = {\n",
    "    \"train\": train_images,\n",
    "    \"val\": val_images,\n",
    "    \"test\": test_images\n",
    "}\n",
    "\n",
    "# Copy image files\n",
    "for split, images in splits_map.items():\n",
    "    for image_path in images:\n",
    "        label_path = label_dir / (image_path.stem + \".txt\")\n",
    "        shutil.copy(image_path, target_dir / \"images\" / split / image_path.name)\n",
    "        shutil.copy(label_path, target_dir / \"labels\" / split / label_path.name)\n",
    "\n",
    "print(f\"Split done! Total = {n_total}\")\n",
    "\n",
    "# Remove directory with unsorted labels\n",
    "if label_dir.exists() and label_dir.is_dir():\n",
    "    shutil.rmtree(label_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773382b3",
   "metadata": {},
   "source": [
    "# YOLOv5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c8b824",
   "metadata": {},
   "source": [
    "### Download pretrained YOLOv5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f60bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5\n",
    "%cd yolov5\n",
    "!pip install -r requirements.txt\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa6db70",
   "metadata": {},
   "source": [
    "### Train the YOLOv5 model on the DeepLesion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5233598",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python yolov5/train.py --img 512 --batch 8 --epochs 100 --data data/deeplesion/deeplesion_yolo/deeplesion.yaml --weights yolov5s.pt --name deeplesion_yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5e99df",
   "metadata": {},
   "source": [
    "### Evaluate the YOLOv5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e2b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python yolov5/val.py --data data/deeplesion/deeplesion_yolo/deeplesion.yaml --weights yolov5/runs/train/deeplesion_yolov5/weights/best.pt --img 512 --task test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd31f2b",
   "metadata": {},
   "source": [
    "# Faster R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cad7ac",
   "metadata": {},
   "source": [
    "### Load Pre-trained Faster R-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff0c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes (no. dataset classes + 1 for background)\n",
    "num_classes = 8 + 1\n",
    "class_names = [\"bone\", \"abdomen\", \"mediastinum\", \"liver\", \"lung\", \"kidney\", \"soft_tissue\", \"pelvis\"]\n",
    "\n",
    "# Set up the available device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "def construct_fasterrcnn_model():\n",
    "    # 1) Load COCO-pretrained Faster R-CNN\n",
    "    weights = FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
    "    model = fasterrcnn_resnet50_fpn_v2(weights=weights, min_size=512, max_size=512)\n",
    "\n",
    "    # 2) Replace the detection head to match the DeepLesion's number of classes\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # 3) Patch the first conv layer to accept 1-channel input\n",
    "    #    (model.backbone.body is the ResNet-50 backbone)\n",
    "    old_conv = model.backbone.body.conv1 # shape: [out_c, 3, k, k] ( == [out_channels, in_channels, kernel_height, kernel_width])\n",
    "\n",
    "    new_conv = nn.Conv2d(\n",
    "        in_channels=1,\n",
    "        out_channels=old_conv.out_channels,\n",
    "        kernel_size=old_conv.kernel_size,\n",
    "        stride=old_conv.stride,\n",
    "        padding=old_conv.padding,\n",
    "        bias=False,\n",
    "    )\n",
    "\n",
    "    # Initialize 1-channel conv layer using pretrained RGB weights\n",
    "    with torch.no_grad():\n",
    "        # Option A: simple average over RGB\n",
    "        new_conv.weight[:] = old_conv.weight.mean(dim=1, keepdim=True)\n",
    "\n",
    "        # Option B (recommended): luminance-weighted sum to mimic grayscale\n",
    "        r = old_conv.weight[:, 0:1, :, :]\n",
    "        g = old_conv.weight[:, 1:2, :, :]\n",
    "        b = old_conv.weight[:, 2:3, :, :]\n",
    "        new_conv.weight[:] = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "    model.backbone.body.conv1 = new_conv\n",
    "\n",
    "    # 4) Adjust the model's internal normalization to 1 channel\n",
    "    # If the loader returns tensors in [0, 1], this centers to roughly ImageNet-like scale.\n",
    "    # [!] If we already pre-normalize to [0, 1] and don't want extra normalization, use mean = [0.0] and std = [1.0].\n",
    "    model.transform.image_mean = [0.5]\n",
    "    model.transform.image_std = [0.5]\n",
    "    # These lists above usually contain 3 values, each for normalization of every RGB channel.\n",
    "    # Since I have only one channel, then I need only one such value in both of these lists.\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    model.to(device)\n",
    "\n",
    "    # Sanity check\n",
    "    # print(f\"First conv layer shape: {model.backbone.body.conv1.weight.shape}\") # Should be [64, 1, 7, 7]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfdcc68",
   "metadata": {},
   "source": [
    "### Prepare DeepLesion dataset for Faster R-CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b26fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for DeepLesion dataset\n",
    "class DeepLesionDataset(Dataset):\n",
    "    def __init__(self, root, split):\n",
    "        # Initialize dataset path, split and transformations\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.transforms = T.Compose([\n",
    "            T.ToTensor(), # Converts [0, 255] uint8 values to float [0.0, 1.0], and preservers 1 channel\n",
    "        ])\n",
    "\n",
    "        # Dataset logic (image paths, annotations, etc.)\n",
    "        self.image_dir = os.path.join(root, \"images\", split)\n",
    "        self.label_dir = os.path.join(root, \"labels\", split)\n",
    "        self.image_names = [img for img in os.listdir(self.image_dir) if img.endswith(\".png\") or img.endswith(\".jpg\")]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_names[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        label_path = os.path.join(self.label_dir, os.path.splitext(image_name)[0] + \".txt\")\n",
    "\n",
    "        image = Image.open(image_path).convert(\"L\")\n",
    "\n",
    "        # Load corresponding bounding boxes and labels\n",
    "        boxes, labels = [], []\n",
    "        if os.path.exists(label_path):\n",
    "            for line in open(label_path):\n",
    "                cls, x_min, y_min, x_max, y_max = map(float, line.split())\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(int(cls))\n",
    "\n",
    "        # Create a target dictionary\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc32327",
   "metadata": {},
   "source": [
    "### Prepare DataLoader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e96ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Shuffling is enabled for training DataLoader, because SGD benefits from seeing data in a new random order every epoch.\n",
    "  During validation and testing phases we do not need that, the order does not affect the metrics.\n",
    "\n",
    "- num_workers is the number of background processes that load & transorm batches in parallel. Good rule of thumb is num_workers being 2-4.\n",
    "\n",
    "- pin_memory, or pinned (page-locked) host memory, speeds up host to GPU copies and lets us use asynchronous transfers\n",
    "  It should be set to True if we train on GPU. It usually gives a smal lbut real throughput bump. It consumes a bit more system RAM\n",
    "  and is useless on CPU-only runs.\n",
    "\n",
    "- Detection models expect lists of images and lists of target dicts, because each image can have different size and has a different\n",
    "  number of boxes. The default PyTorch collate tries to stack everything into tensors of the same shape, which breaks for \n",
    "  variable-length targets. Custom collate_fn function here unzips the list oof pairs into pair of lists so Faster R-CNN can consume them:\n",
    "    images: List[Tensor[C,H,W]]\n",
    "    targets: List[Dict{'boxes': Tensor[N,4], 'labels': Tensor[N]}]\n",
    "  That is exactly what torchvision's detection references use.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "deeplesion_fasterrcnn_path = \"deeplesion_fasterrcnn_split_1\"\n",
    "train_ds = DeepLesionDataset(deeplesion_data_dir / deeplesion_fasterrcnn_path, \"train\")\n",
    "val_ds = DeepLesionDataset(deeplesion_data_dir / deeplesion_fasterrcnn_path, \"val\")\n",
    "test_ds = DeepLesionDataset(deeplesion_data_dir / deeplesion_fasterrcnn_path, \"test\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch: [(img1, target1), (img2, target2), ...]\n",
    "    # returns: ([img1, img2, ...], [target1, target2, ...])\n",
    "    return tuple(zip(*batch)) # -> \n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=2, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=2, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2014798",
   "metadata": {},
   "source": [
    "### Evaluation Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78e477c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_detector(model, loader, device, num_classes=8, class_names=None, pr_conf_thr=0.25, pr_iou_thr=0.5):\n",
    "    \"\"\"\n",
    "        Returns a dictionary:\n",
    "        {\n",
    "            \"mAP50\": float,\n",
    "            \"mAP50_95: float,\n",
    "            \"per_class\": [\"name\": ..., \"AP50\": ..., \"AP\": ...],\n",
    "            \"precision_overall\": float,\n",
    "            \"recall_overall\": float,\n",
    "            \"precision_per_class\": [...],\n",
    "            \"recall_per_class\": [...],\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision(iou_type=\"bbox\", box_format=\"xyxy\", class_metrics=True)\n",
    "\n",
    "    # For precision and recall at fixed thresholds (IoU=0.5, conf=pr_conf_thr)\n",
    "    TP = torch.zeros(num_classes, dtype=torch.long)\n",
    "    FP = torch.zeros(num_classes, dtype=torch.long)\n",
    "    FN = torch.zeros(num_classes, dtype=torch.long)\n",
    "\n",
    "    for images, targets in loader:\n",
    "        images = [image.to(device) for image in images]\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Move to CPU for metrics\n",
    "        predictions, ground_truths = [], []\n",
    "        for output, target in zip(outputs, targets):\n",
    "            predictions.append({\"boxes\": output[\"boxes\"].cpu(),\n",
    "                                \"scores\": output[\"scores\"].cpu(),\n",
    "                                \"labels\": output[\"labels\"].cpu()})\n",
    "            ground_truths.append({\"boxes\": target[\"boxes\"].cpu(),\n",
    "                                  \"labels\": target[\"labels\"].cpu()})\n",
    "            \n",
    "        # mAP update\n",
    "        metric.update(predictions, ground_truths)\n",
    "\n",
    "        # Precision and recall accumulation at fixed thresholds\n",
    "        for output, target in zip(predictions, ground_truths):\n",
    "            # Filter predictions by confidence\n",
    "            keep = output[\"scores\"] >= pr_conf_thr\n",
    "            pred_boxes = output[\"boxes\"][keep]\n",
    "            pred_labels = output[\"labels\"][keep]\n",
    "            gt_boxes = target[\"boxes\"]\n",
    "            gt_labels = target[\"labels\"]\n",
    "\n",
    "            matched = torch.zeros(len(gt_boxes), dtype=torch.bool)\n",
    "            if len(pred_boxes) and len(gt_boxes):\n",
    "                ious = ops.box_iou(pred_boxes, gt_boxes)\n",
    "                for pred_idx in range(len(pred_boxes)):\n",
    "                    cls = int(pred_labels[pred_idx].item()) # classes are 1...K\n",
    "                    # candidates: same class\n",
    "                    same = (gt_labels == cls)\n",
    "                    if same.any():\n",
    "                        ious_c = ious[pred_idx, same]\n",
    "                        if len(ious_c):\n",
    "                            gt_idxs = torch.where(same)[0]\n",
    "                            best_iou, best_loc = ious_c.max(0)\n",
    "                            gt_idx = gt_idxs[best_loc]\n",
    "                            if best_iou >= pr_iou_thr and not matched[gt_idx]:\n",
    "                                TP[cls - 1] += 1\n",
    "                                matched[gt_idx] = True\n",
    "                            else:\n",
    "                                FP[cls - 1] += 1\n",
    "                        else:\n",
    "                            FP[cls - 1] += 1\n",
    "                    else:\n",
    "                        FP[cls - 1] += 1\n",
    "            \n",
    "            # Any unmatched ground truths are FN\n",
    "            for gt_idx, gt_label in enumerate(gt_labels):\n",
    "                if not matched[gt_idx]:\n",
    "                    FN[int(gt_label.item()) - 1] += 1\n",
    "\n",
    "        # mAP metrics\n",
    "        res = metric.compute()\n",
    "        out = {\n",
    "            \"mAP50\": float(res[\"map_50\"]),\n",
    "            \"mAP50_95\": float(res[\"map\"]),\n",
    "        }\n",
    "\n",
    "        # Per-class AP (if available)\n",
    "        per_class = []\n",
    "        map_per_class = res.get(\"map_per_class\", None)\n",
    "        map50_per_class = res.get(\"map_50_per_class\", None)\n",
    "        if map_per_class is not None:\n",
    "            ap = map_per_class.tolist()\n",
    "            ap50 = map50_per_class.tolist() if map50_per_class is not None else [None] * len(ap)\n",
    "            for i in range(len(ap)):\n",
    "                name = class_names[i] if class_names and i < len(class_names) else f\"class_{i + 1}\"\n",
    "                per_class.append({\"idx\": i + 1, \"name\": name, \"AP\": ap[i], \"AP50\": ap50[i]})\n",
    "        out[\"per_class\"] = per_class\n",
    "\n",
    "        # Precision/Recall at fixed thresholds\n",
    "        precision_per_class = (TP.float() / (TP + FP).clamp(min=1)).tolist()\n",
    "        recall_per_class = (TP.float() / (TP + FN).clamp(min=1)).tolist()\n",
    "        overall_precision = float(TP.sum() / (TP.sum() + FP.sum()).clamp(min=1))\n",
    "        overall_recall = float(TP.sum() / (TP.sum() + FN.sum()).clamp(min=1))\n",
    "\n",
    "        out[\"precision_overall\"] = overall_precision\n",
    "        out[\"recall_overall\"] = overall_recall\n",
    "        out[\"precision_per_class\"] = precision_per_class\n",
    "        out[\"recall_per_class\"] = recall_per_class\n",
    "        out[\"pr_conf_thr\"] = pr_conf_thr\n",
    "        out[\"pr_iou_thr\"] = pr_iou_thr\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4af3f6",
   "metadata": {},
   "source": [
    "### Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92f4e041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training started ***\n",
      "First conv layer shape: torch.Size([64, 1, 7, 7])\n",
      "*** Epoch 1/20 started ***\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000389_05_01_111.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000389_05_01_111.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000832_02_01_015.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000832_02_01_015.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/003211_04_01_321.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/003211_04_01_321.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000187_05_01_039.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000187_05_01_039.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000569_08_02_076.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000569_08_02_076.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000204_10_01_058.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/003118_01_02_038.pngdata/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000204_10_01_058.txt\n",
      "\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/003118_01_02_038.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/003855_01_01_059.pngdata/deeplesion/deeplesion_fasterrcnn_split_1/images/train/003877_02_01_068.png\n",
      "\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/003855_01_01_059.txtdata/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/003877_02_01_068.txt\n",
      "\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000693_02_01_044.pngdata/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000468_01_01_021.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000468_01_01_021.txt\n",
      "\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000693_02_01_044.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000607_01_01_121.pngdata/deeplesion/deeplesion_fasterrcnn_split_1/images/train/001103_04_01_120.png\n",
      "\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000607_01_01_121.txtdata/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/001103_04_01_120.txt\n",
      "\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/001006_02_01_011.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/002842_02_01_023.pngdata/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/001006_02_01_011.txt\n",
      "\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/002842_02_01_023.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/002734_02_01_116.pngdata/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000961_04_01_080.png\n",
      "\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/002734_02_01_116.txtdata/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000961_04_01_080.txt\n",
      "\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/003722_04_01_040.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/003722_04_01_040.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/003066_01_02_388.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/003066_01_02_388.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/002041_01_01_102.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/002041_01_01_102.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000046_01_01_044.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000046_01_01_044.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/001937_03_02_243.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/001937_03_02_243.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/003599_01_01_001.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/003599_01_01_001.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/002970_05_01_026.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/002970_05_01_026.txt\n",
      "{'loss_classifier': tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(nan, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/001115_02_01_025.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/001115_02_01_025.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/002701_05_02_172.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/002701_05_02_172.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/002805_04_01_120.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/002805_04_01_120.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/001103_01_01_121.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/001103_01_01_121.txt\n",
      "{'loss_classifier': tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(nan, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/001209_01_01_037.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/001209_01_01_037.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/001059_04_02_216.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/001059_04_02_216.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000909_07_02_149.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000909_07_02_149.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000865_01_01_126.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000865_01_01_126.txt\n",
      "{'loss_classifier': tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(nan, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/002648_02_01_357.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/002648_02_01_357.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/001215_01_01_021.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/001215_01_01_021.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/001670_01_02_072.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/001670_01_02_072.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/001343_01_01_033.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/001343_01_01_033.txt\n",
      "{'loss_classifier': tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(nan, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/001751_03_01_075.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/001751_03_01_075.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/002402_04_01_032.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/002402_04_01_032.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000586_08_01_046.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000586_08_01_046.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/001585_01_01_030.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/001585_01_01_030.txt\n",
      "{'loss_classifier': tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(nan, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/001043_03_02_394.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/001043_03_02_394.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000230_04_01_242.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000230_04_01_242.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/003211_01_01_552.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/003211_01_01_552.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000020_04_01_143.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000020_04_01_143.txt\n",
      "{'loss_classifier': tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(nan, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000772_05_01_044.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000772_05_01_044.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/003991_01_03_084.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/003991_01_03_084.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/001301_01_01_283.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/001301_01_01_283.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/001959_01_01_065.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/001959_01_01_065.txt\n",
      "{'loss_classifier': tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(nan, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/002946_01_01_123.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/002946_01_01_123.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000420_01_01_054.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000420_01_01_054.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/001023_05_01_091.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/001023_05_01_091.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/002680_05_01_305.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/002680_05_01_305.txt\n",
      "{'loss_classifier': tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(nan, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/003247_01_01_126.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/003247_01_01_126.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/003722_06_01_051.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/003722_06_01_051.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000280_03_01_159.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000280_03_01_159.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/001925_04_03_093.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/001925_04_03_093.txt\n",
      "{'loss_classifier': tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(nan, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/002614_05_01_089.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/002614_05_01_089.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/002741_01_01_019.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/002741_01_01_019.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/003877_02_01_077.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/003877_02_01_077.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000992_03_01_034.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000992_03_01_034.txt\n",
      "{'loss_classifier': tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(nan, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 1/20, Batch: [10/1684], Images: [40/6736], Loss: nan\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000229_03_04_153.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000229_03_04_153.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/001361_01_01_035.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/001361_01_01_035.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/001983_04_01_077.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/001983_04_01_077.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/003663_02_03_078.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/003663_02_03_078.txt\n",
      "{'loss_classifier': tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(nan, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/003185_06_02_555.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/003185_06_02_555.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/001029_01_02_015.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/001029_01_02_015.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/002741_01_01_021.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/002741_01_01_021.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000238_01_01_024.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000238_01_01_024.txt\n",
      "{'loss_classifier': tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(nan, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000457_01_01_052.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000457_01_01_052.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/000026_04_01_339.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/000026_04_01_339.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/002958_01_02_117.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/002958_01_02_117.txt\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/images/train/002944_11_01_340.png\n",
      "data/deeplesion/deeplesion_fasterrcnn_split_1/labels/train/002944_11_01_340.txt\n",
      "{'loss_classifier': tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(nan, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 145\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m learning_rate \u001b[38;5;129;01min\u001b[39;00m learning_rates:\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m weight_decay \u001b[38;5;129;01min\u001b[39;00m weight_decays:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m         result = train_one_config(\n\u001b[32m    146\u001b[39m             train_loader=train_loader, val_loader=val_loader, device=device,\n\u001b[32m    147\u001b[39m             learning_rate=learning_rate, weight_decay=weight_decay, max_epochs=max_epochs, patience=patience,\n\u001b[32m    148\u001b[39m             metric_key=\u001b[33m\"\u001b[39m\u001b[33mmAP50_95\u001b[39m\u001b[33m\"\u001b[39m, use_amp=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    149\u001b[39m         )\n\u001b[32m    151\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m best_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m result[\u001b[33m\"\u001b[39m\u001b[33mbest_metric\u001b[39m\u001b[33m\"\u001b[39m] > best_result[\u001b[33m\"\u001b[39m\u001b[33mbest_metric\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    152\u001b[39m             best_result = result\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mtrain_one_config\u001b[39m\u001b[34m(train_loader, val_loader, device, learning_rate, weight_decay, momentum, max_epochs, patience, metric_key, use_amp, gamma, step_size)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_amp:\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=(device.type == \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m), cache_enabled=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m         loss_dict = model(images, targets)\n\u001b[32m     79\u001b[39m         loss = \u001b[38;5;28msum\u001b[39m(loss_dict.values())\n\u001b[32m     80\u001b[39m     scaler.scale(loss).backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lesion_detector_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lesion_detector_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lesion_detector_env/lib/python3.12/site-packages/torchvision/models/detection/generalized_rcnn.py:104\u001b[39m, in \u001b[36mGeneralizedRCNN.forward\u001b[39m\u001b[34m(self, images, targets)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch.Tensor):\n\u001b[32m    103\u001b[39m     features = OrderedDict([(\u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m, features)])\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m proposals, proposal_losses = \u001b[38;5;28mself\u001b[39m.rpn(images, features, targets)\n\u001b[32m    105\u001b[39m detections, detector_losses = \u001b[38;5;28mself\u001b[39m.roi_heads(features, proposals, images.image_sizes, targets)\n\u001b[32m    106\u001b[39m detections = \u001b[38;5;28mself\u001b[39m.transform.postprocess(detections, images.image_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lesion_detector_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lesion_detector_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lesion_detector_env/lib/python3.12/site-packages/torchvision/models/detection/rpn.py:371\u001b[39m, in \u001b[36mRegionProposalNetwork.forward\u001b[39m\u001b[34m(self, images, features, targets)\u001b[39m\n\u001b[32m    367\u001b[39m objectness, pred_bbox_deltas = concat_box_prediction_layers(objectness, pred_bbox_deltas)\n\u001b[32m    368\u001b[39m \u001b[38;5;66;03m# apply pred_bbox_deltas to anchors to obtain the decoded proposals\u001b[39;00m\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# note that we detach the deltas because Faster R-CNN do not backprop through\u001b[39;00m\n\u001b[32m    370\u001b[39m \u001b[38;5;66;03m# the proposals\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m proposals = \u001b[38;5;28mself\u001b[39m.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n\u001b[32m    372\u001b[39m proposals = proposals.view(num_images, -\u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m)\n\u001b[32m    373\u001b[39m boxes, scores = \u001b[38;5;28mself\u001b[39m.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lesion_detector_env/lib/python3.12/site-packages/torchvision/models/detection/_utils.py:178\u001b[39m, in \u001b[36mBoxCoder.decode\u001b[39m\u001b[34m(self, rel_codes, boxes)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m box_sum > \u001b[32m0\u001b[39m:\n\u001b[32m    177\u001b[39m     rel_codes = rel_codes.reshape(box_sum, -\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m pred_boxes = \u001b[38;5;28mself\u001b[39m.decode_single(rel_codes, concat_boxes)\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m box_sum > \u001b[32m0\u001b[39m:\n\u001b[32m    180\u001b[39m     pred_boxes = pred_boxes.reshape(box_sum, -\u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lesion_detector_env/lib/python3.12/site-packages/torchvision/models/detection/_utils.py:216\u001b[39m, in \u001b[36mBoxCoder.decode_single\u001b[39m\u001b[34m(self, rel_codes, boxes)\u001b[39m\n\u001b[32m    213\u001b[39m pred_h = torch.exp(dh) * heights[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    215\u001b[39m \u001b[38;5;66;03m# Distance from center to box's corner.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m c_to_c_h = torch.tensor(\u001b[32m0.5\u001b[39m, dtype=pred_ctr_y.dtype, device=pred_h.device) * pred_h\n\u001b[32m    217\u001b[39m c_to_c_w = torch.tensor(\u001b[32m0.5\u001b[39m, dtype=pred_ctr_x.dtype, device=pred_w.device) * pred_w\n\u001b[32m    219\u001b[39m pred_boxes1 = pred_ctr_x - c_to_c_w\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "- For Faster R-CNN it is common to use SGD or Adam as the optimizer.\n",
    "- Hyperparameters:\n",
    "    - momentum: \n",
    "        adds an exponential moving average of past gradients to the current step, which causes smoother updates,\n",
    "        less zig-zagging and faster convergance. Typically set to 0.9, and rarely needs tuning.\n",
    "    - weight_decay (L2 regularization):\n",
    "        Penalizes large weights to reduce overfitting (shrinks params each step).\n",
    "        Typical for detection with SGD: 5e-4 or 1e-4\n",
    "    - step_size (in StepLR):\n",
    "        Every step_size epochs, the LR scheduler triggers a decay.\n",
    "    - gamma (in StepLR):\n",
    "        Multiplicative LR factor at each step: new_lr = old_lr * gamma. Commonly set to 0.1.\n",
    "\n",
    "- Cross-validate only on the following hyperparameters:\n",
    "    - LR: [0.01, 0.005, 0.002]\n",
    "    - weight_decay: [5e-4, 1e-4]\n",
    "    - Epochs -> don't cross-validate over it -> set a generous cap (e.g. 20) and early stop\n",
    "\n",
    "- use_amp (AMP - Automatic Mixed Precision) - runs many ops in float16 instead of float32, which takes much less GPU memory and is often faster\n",
    "- autoca\n",
    "\n",
    "- autocast() is a context manager that automatically picks a safe dtype per op (keeps numerically sensitive ops in float32, others in float16).\n",
    "  It saves memory/computation.\n",
    "  \n",
    "- GradScaler multiplies the loss by a large scale before backprop to avoid float16 underflow, then unscales safely before the optimizer step.\n",
    "  It makes the gradients stable in half precision.\n",
    "\n",
    "- optimizer.zero_grad(set_to_none=True) - set_to_none parameter set to True means that for each parameter param.grad is set to None\n",
    "  (no tensor is kept). On the next backward() PyTorch allocates a fresh grad tensor and writes into it. It causes faster & less memory traffic\n",
    "  by avoiding writing zeros over large grad buffers every step. Lowers memory footprint by letting unused grads be garbage-collected and reallocated\n",
    "  only when needed.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def train_one_config(\n",
    "    train_loader, val_loader, device,\n",
    "    learning_rate, weight_decay, momentum=0.9,\n",
    "    max_epochs=20, patience=5, metric_key=\"mAP50_95\",\n",
    "    use_amp=True, gamma=0.1, step_size=3\n",
    "):\n",
    "    # Construct the model\n",
    "    model = construct_fasterrcnn_model()\n",
    "\n",
    "    # Set up optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    # Set up scaler\n",
    "    use_amp = use_amp and (device.type == \"cuda\")\n",
    "    scaler = torch.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    best_metric = -float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "    history = []\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        print(f\"*** Epoch {epoch}/{max_epochs} started ***\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_processed_images = 0\n",
    "\n",
    "        # Training loop\n",
    "        for batch_idx, (images, targets) in enumerate(train_loader, start=1):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{key: val.to(device) for key, val in target.items()} for target in targets]\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if use_amp:\n",
    "                with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=(device.type == \"cuda\"), cache_enabled=True):\n",
    "                    loss_dict = model(images, targets)\n",
    "                    loss = sum(loss_dict.values())\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss_dict = model(images, targets)\n",
    "                loss = sum(loss_dict.values())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print(loss_dict)\n",
    "            running_loss += loss.item()\n",
    "            n_processed_images += len(images)\n",
    "\n",
    "            if batch_idx % 10 == 0 or batch_idx == len(train_loader):\n",
    "                avg_loss_so_far = running_loss / batch_idx\n",
    "                print(f\"Epoch: {epoch}/{max_epochs}, Batch: [{batch_idx}/{len(train_loader)}], \"\n",
    "                    f\"Images: [{n_processed_images}/{len(train_ds)}], Loss: {avg_loss_so_far:.4f}\")\n",
    "            \n",
    "        lr_scheduler.step()\n",
    "        train_loss = running_loss / max(1, len(train_loader))\n",
    "        print(f\"*** Epoch {epoch}/{max_epochs} finished -> Loss: {train_loss:.4f} ***\")\n",
    "\n",
    "        # Validation\n",
    "        print(\"*** Validation started ***\")\n",
    "        val_metrics = evaluate_detector(model, val_loader, device, num_classes, class_names)\n",
    "        val_score = float(val_metrics[metric_key])\n",
    "\n",
    "        history.append({\"epoch\": epoch, \"train_loss\": train_loss, **val_metrics})\n",
    "        print(f\"Loss={train_loss:.4f}, mAP50={val_metrics[\"mAP50\"]:.4f}, mAP50-95={val_metrics[\"mAP50-95\"]:.4f}\")\n",
    "        print(f\"*** Epoch {epoch}/{max_epochs} finished ***\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_score > best_metric + 1e-6:\n",
    "            best_metric = val_score\n",
    "            best_epoch = epoch\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"*** Early stopping after {epoch}/{max_epochs} epochs (best at {best_epoch} with {metric_key}={best_metric:.4f}). ***\")\n",
    "                break\n",
    "\n",
    "    return {\n",
    "        \"best_metric\": best_metric,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"best_state\": best_state,\n",
    "        \"history\": history,\n",
    "        \"lr\": learning_rate,\n",
    "        \"weight_decay\": weight_decay\n",
    "    }\n",
    "\n",
    "# =================================================================================================================================================\n",
    "# =================================================================================================================================================\n",
    "\n",
    "# Set up hyperparameters\n",
    "learning_rates = [0.01, 0.005, 0.002]\n",
    "weight_decays = [1e-4, 5e-4]\n",
    "max_epochs = 20\n",
    "patience = 5\n",
    "best_result = None\n",
    "\n",
    "print(\"*** Training started ***\")\n",
    "for learning_rate in learning_rates:\n",
    "    for weight_decay in weight_decays:\n",
    "        result = train_one_config(\n",
    "            train_loader=train_loader, val_loader=val_loader, device=device,\n",
    "            learning_rate=learning_rate, weight_decay=weight_decay, max_epochs=max_epochs, patience=patience,\n",
    "            metric_key=\"mAP50_95\", use_amp=True\n",
    "        )\n",
    "\n",
    "        if best_result is None or result[\"best_metric\"] > best_result[\"best_metric\"]:\n",
    "            best_result = result\n",
    "\n",
    "print(\"*** Training complete ***\")\n",
    "print(f\"Best config: lr={best_result[\"lr\"]} wd={best_result[\"weight_decay\"]} \"\n",
    "      f\"epoch={best_result[\"best_epoch\"]} mAP50-95={best_result[\"best_metric\"]:.4f}\")\n",
    "\n",
    "# Save the best model checkpoint\n",
    "best_checkpoint = {\n",
    "    \"state_dict\": best_result[\"best_state\"],\n",
    "    \"epoch\": best_result[\"best_epoch\"],\n",
    "    \"metric_key\": \"mAP50-95\",\n",
    "    \"metric_value\": best_result[\"best_metric\"],\n",
    "    \"hp\": {\n",
    "        \"lr\": best_result[\"lr\"],\n",
    "        \"weight_decay\": best_result[\"weight_decay\"],\n",
    "        \"momentum\": 0.9,\n",
    "        \"step_size\": 3,\n",
    "        \"gamma\": 0.1,\n",
    "        \"max_epochs\": max_epochs,\n",
    "        \"patience\": patience,\n",
    "    },\n",
    "    # Describe how to reconstruct the model\n",
    "    \"model_spec\": {\n",
    "        \"arch\": \"fasterrcnn_resnet50_fpn_v2\",\n",
    "        \"min_size\": 512,\n",
    "        \"max_size\": 512,\n",
    "        \"in_channels\": 1,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"image_mean\": [0.5],\n",
    "        \"image_std\": [0.5],\n",
    "    },\n",
    "    \"class_names\": class_names,\n",
    "    \"versions\": {\"torch\": torch.__version__, \"torchvision\": torchvision.__version__},\n",
    "}\n",
    "\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "save_path = f\"checkpoints/fasterrcnn_best_{ts}.pt\"\n",
    "torch.save(best_checkpoint, save_path)\n",
    "print(f\"Saved best checkpoint to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bccbb",
   "metadata": {},
   "source": [
    "### Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576ca648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best weights and evaluate on the test set\n",
    "best_checkpoint = torch.load(save_path, map_location=\"cpu\")\n",
    "best_model = construct_fasterrcnn_model()\n",
    "best_model.load_state_dict(best_checkpoint[\"state_dict\"])\n",
    "\n",
    "# Evaluate the best model\n",
    "print(f\"*** Evaluation started ***\")\n",
    "test_metrics = evaluate_detector(best_model, test_loader, device, num_classes, class_names)\n",
    "\n",
    "print(f\"*** Evaluation finished ***\")\n",
    "print(test_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lesion_detector_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
