{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad82646",
   "metadata": {},
   "source": [
    "# Supervised Lesion Detector\n",
    "\n",
    "This notebook is dedicated to training and evaluating a supervised lesion detector on DeepLesion dataset with the following supervised model architectures for image detection with ResNet-50 backbone:\n",
    "- YOLOv5 (stable, but YOLOv8 is newer),\n",
    "- Faster R-CNN (torchvision.models.detection or Detectron2),\n",
    "- DETR (Facebook DETR),\n",
    "- Improved DETR (DINO-DETR or Deformable DETR, DINO has better performance but Deformable is faster),\n",
    "- RetinaNet (FPN backbone + anchor-based).\n",
    "\n",
    "## Assumptions:\n",
    "- Use 2D slice inputs (optionally use the neighbouring ones too),\n",
    "- Resize all images to 512x512,\n",
    "- Use COCO-style Dataset class.\n",
    "- Use DeepLesion for training a general lesion localizer and some other like LiTS (Liver Tumor Segmentation) or CHAOS (CT liver dataset) for more specialized localizer.\n",
    "\n",
    "## ðŸ“š Thesis Value Summary\n",
    "### Contribution and Value:\n",
    "- Comparison of CNN vs Transformer detectors on DeepLesion\t-> âœ… Fills a gap in literature\n",
    "- Evaluation of improved DETRs (DINO/Deformable) -> âœ… Modern insight\n",
    "- General vs specialized lesion detection -> âœ… Strong clinical relevance\n",
    "- Analysis of training time, robustness, failure modes -> âœ… Engineering depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc32778b",
   "metadata": {},
   "source": [
    "# Image utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3afee5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.typing import NDArray\n",
    "from typing import Any\n",
    "from PIL import Image\n",
    "\n",
    "def load_metadata(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads metadata from the given path and\n",
    "    returns it as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def normalize(img: NDArray[np.uint16], per_image_norm: bool):\n",
    "    \"\"\"\n",
    "    Normalizes the input image\n",
    "    \"\"\"\n",
    "\n",
    "    img = img.astype(np.float32)\n",
    "    if not per_image_norm:\n",
    "        return img / 65535.0\n",
    "    max = np.max(img)\n",
    "    min = np.min(img)\n",
    "    img = (img - min) / (max - min)\n",
    "    return img\n",
    "\n",
    "def convert_to_hu(img: NDArray[np.uint16], norm: bool, hu_min=-1024, hu_max=3071):\n",
    "    \"\"\"\n",
    "    Converts the pixel data of a uint16\n",
    "    CT image to Hounsfield Units (HU).\n",
    "    \"\"\"\n",
    "\n",
    "    hu_img = img.astype(np.int32) - 32768\n",
    "    hu_img = np.clip(hu_img, hu_min, hu_max).astype(np.float32)\n",
    "    if norm:\n",
    "        hu_img = (hu_img - hu_min) / (hu_max - hu_min)\n",
    "        hu_img = np.clip(hu_img, 0.0, 1.0)\n",
    "    return hu_img\n",
    "\n",
    "def load_image(path: str, hu_scale: bool = True, norm: bool = True, per_image_norm: bool = True):\n",
    "    \"\"\"\n",
    "    Loads an image given its path\n",
    "    and returns it as a numpy array.\n",
    "    \"\"\"\n",
    "    \n",
    "    img = Image.open(path)\n",
    "    img_array = np.array(img)\n",
    "    if hu_scale:\n",
    "        hu_min = -160\n",
    "        hu_max = 240\n",
    "        return convert_to_hu(img_array, norm, hu_min, hu_max)\n",
    "    elif norm:\n",
    "        return normalize(img_array, per_image_norm)\n",
    "    return img_array\n",
    "\n",
    "def save_image(img_array: NDArray[Any], path: str):\n",
    "    \"\"\"\n",
    "    Saves the input image to the specified path.\n",
    "    \"\"\"\n",
    "\n",
    "    if img_array.dtype != np.uint16:\n",
    "        img_array = img_array.astype(np.uint16)\n",
    "    img = Image.fromarray(img_array)\n",
    "    img.save(path)\n",
    "\n",
    "def show_image(img: NDArray[np.float32], title=\"Example Image\", cmap=\"gray\"):\n",
    "    \"\"\"\n",
    "    Shows the image given its data, title and colour map.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc4fda",
   "metadata": {},
   "source": [
    "# Convert DeepLesion metadata to COCO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf34ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "deeplesion_metadata = load_metadata(\"../data/deeplesion_metadata.csv\")\n",
    "\n",
    "images = []\n",
    "annotations = []\n",
    "categories = [\n",
    "    {\"id\": 1, \"name\": \"bone\"},\n",
    "    {\"id\": 2, \"name\": \"abdomen\"},\n",
    "    {\"id\": 3, \"name\": \"mediastinum\"},\n",
    "    {\"id\": 4, \"name\": \"liver\"},\n",
    "    {\"id\": 5, \"name\": \"lung\"},\n",
    "    {\"id\": 6, \"name\": \"kidney\"},\n",
    "    {\"id\": 7, \"name\": \"soft tissue\"},\n",
    "    {\"id\": 8, \"name\": \"pelvis\"}\n",
    "]\n",
    "\n",
    "image_id = 1\n",
    "annotation_id = 1\n",
    "\n",
    "for idx, row in deeplesion_metadata.iterrows():\n",
    "    # Extract only images with annotated lesions (Val + Test)\n",
    "    if row[\"Train_Val_Test\"] == 1:\n",
    "        continue\n",
    "\n",
    "    file_name = row[\"File_name\"]\n",
    "    lesion_type = row[\"Coarse_lesion_type\"]\n",
    "    bbox_str = row[\"Bounding_boxes\"]\n",
    "    size_str = row[\"Image_size\"]\n",
    "    image_path = os.path.join(\"../data/deeplesion/key_slices/\", file_name)\n",
    "\n",
    "    # Extract ground truth bounding box' coordinates\n",
    "    bbox_coords = [float(val) for val in bbox_str.split(\",\")]\n",
    "    x1, y1, x2, y2 = [round(c) for c in bbox_coords]\n",
    "\n",
    "    # Extract sizes\n",
    "    bbox_sizes = [int(val) for val in size_str.split(\",\")]\n",
    "    width, height = [size for size in bbox_sizes]\n",
    "\n",
    "    # Register image\n",
    "    images.append({\n",
    "        \"id\": image_id,\n",
    "        \"file_name\": file_name,\n",
    "        \"width\": width,\n",
    "        \"height\": height\n",
    "    })\n",
    "\n",
    "    # Initialize ground truth bounding box' parameters\n",
    "    bbox = [x1, y1, x2 - x1, y2 - y1]\n",
    "    area = bbox[2] * bbox[3]\n",
    "\n",
    "    annotations.append({\n",
    "        \"id\": annotation_id,\n",
    "        \"image_id\": image_id,\n",
    "        \"category\": lesion_type,\n",
    "        \"bbox\": bbox,\n",
    "        \"area\": area,\n",
    "        \"iscrowd\": 0    # Normal object (not crowd of indistinct objects, that can't be cleanly separated)\n",
    "    })\n",
    "\n",
    "    annotation_id += 1\n",
    "    image_id += 1\n",
    "\n",
    "# Save to JSON\n",
    "deeplesion_coco_format = {\n",
    "    \"images\": images,\n",
    "    \"annotations\": annotations,\n",
    "    \"categories\": categories\n",
    "}\n",
    "\n",
    "with open(\"../data/deeplesion_coco.json\", \"w\") as f:\n",
    "    json.dump(deeplesion_coco_format, f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773382b3",
   "metadata": {},
   "source": [
    "# Train YOLOv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f60bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert COCO JSON to YOLOv5 .txt format\n",
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lesion_detector_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
